{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e487e0",
   "metadata": {},
   "source": [
    "# core.dom\n",
    "\n",
    ">DOM Model with Pydantic and Pandoc Integration\n",
    "output-file: core.dom.html\n",
    "title: core.dom\n",
    "\n",
    "This notebook demonstrates a Document Object Model (DOM) using Pydantic for static typing and validation, and integrates Pandoc (via pypandoc) for Markdown processing.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17235612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import List, Optional, ClassVar\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from functools import cached_property\n",
    "import base64\n",
    "import pypandoc\n",
    "import pathlib\n",
    "import asyncio\n",
    "import tqdm\n",
    "import json\n",
    "import jsoncfg\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import markdown\n",
    "import asyncio \n",
    "import tqdm\n",
    "import hashlib\n",
    "from functools import cached_property\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.api import AsyncClientAPI, ClientAPI\n",
    "from chromadb.api.models.Collection import Collection\n",
    "from enum import Enum\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361567c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from ollama import chat, ChatResponse, Client, AsyncClient\n",
    "from openai import OpenAI, AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bf070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OLLAMA_API_KEY= os.getenv('OLLAMA_API_KEY')\n",
    "print(OLLAMA_API_KEY)\n",
    "DASHSCOPE_API_KEY= os.getenv('DASHSCOPE_API_KEY')\n",
    "print(DASHSCOPE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e78a35",
   "metadata": {},
   "source": [
    "## Base Element Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837951d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Element(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a DOM element with a tag, attributes, and children.\n",
    "    \"\"\"\n",
    "    summary: Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18b3b4",
   "metadata": {},
   "source": [
    "## Figure Class with Base64 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Figure(Element):\n",
    "    \"\"\"\n",
    "    Represents a figure element in the document.\n",
    "    \"\"\"\n",
    "    rawdata: str = Field(..., description=\"Base64-encoded image data\")\n",
    "\n",
    "    @field_validator(\"rawdata\")\n",
    "    def validate_base64(cls, v):\n",
    "        \"\"\" \n",
    "        Validates that the rawdata is a valid base64-encoded string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            base64.b64decode(v)\n",
    "        except Exception:\n",
    "            raise ValueError(\"rawdata must be valid base64\")\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc259b",
   "metadata": {},
   "source": [
    "## Table Structure: Cell, Column, Row, Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Cell(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a table cell in the document.\n",
    "    \"\"\"\n",
    "    c: str\n",
    "\n",
    "\n",
    "class Column(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a table column in the document.\n",
    "    \"\"\"\n",
    "    cells: List[Cell]\n",
    "\n",
    "\n",
    "class Row(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a table row in the document.\n",
    "    \"\"\"\n",
    "    cols: List[Column]\n",
    "\n",
    "\n",
    "class Table(Element):\n",
    "    \"\"\"\n",
    "    Represents a table element in the document.\n",
    "    \"\"\"\n",
    "    rows: List[Row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac94f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_text_summary_response(content: str, model:str=\"gemma3-27b\", role:str=\"user\", lang: str=\"zh\") -> ChatResponse:\n",
    "    \"\"\"\n",
    "    Returns a ChatResponse from the chat model with a summary prompt for the given content.\n",
    "    \"\"\"\n",
    "    match lang:\n",
    "        case \"en\":\n",
    "            prompt = (\n",
    "                f\"Please provide a summary of the following string. \"\n",
    "                f\"The summary should be concise and informative: {content}. \"\n",
    "            )\n",
    "        case \"zh\":\n",
    "            content = re.sub(r\"\\s+\", \" \", content.strip())\n",
    "            prompt = (\n",
    "                f\"请提供以下字符串的摘要。\"\n",
    "                f\"摘要应简明扼要且信息丰富: {content}. \"\n",
    "            )\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported language: {lang}\")\n",
    "    return chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": role,\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "image_link_pattern = r'[^\\s]+\\.(?:jpg|jpeg|png|gif|bmp|webp)'\n",
    "\n",
    "def get_image_summary_response(image_link: str | Path, model:str=\"gemma3:27b\", role:str=\"user\", lang: str='zh') -> ChatResponse:\n",
    "    \"\"\"\n",
    "    Returns a ChatResponse from the chat model with a summary prompt for the given image.\n",
    "    \"\"\"\n",
    "    if isinstance(image_link, Path):\n",
    "        image_link = str(image_link)\n",
    "    if not re.match(image_link_pattern, image_link):\n",
    "        # If the image link is not a URL, throw an error\n",
    "        raise ValueError(f\"Invalid image link: {image_link}\")\n",
    "    \n",
    "    match lang:\n",
    "        case 'en':\n",
    "            prompt = \"Please provide a summary of the following image. The summary should be concise and informative about the robot.\"\n",
    "        case 'zh':\n",
    "            prompt = \"请提供以下图像的摘要。关于机器人机械尺寸,运动范围,自由度的说明应简明扼要。\"\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": role,\n",
    "                \"content\": prompt,\n",
    "                'images': [f\"{image_link}\"],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def get_text_summary_response_async(client: AsyncClient, content: str, model:str=\"gemma3-27b\", role:str=\"user\", lang: str=\"zh\") -> ChatResponse:\n",
    "    \"\"\"\n",
    "    Returns a ChatResponse from the chat model with a summary prompt for the given content.\n",
    "    \"\"\"\n",
    "    content = re.sub(r\"\\s+\", \" \", content.strip())\n",
    "    match lang:\n",
    "        case \"en\":\n",
    "            prompt = (\n",
    "                f\"Please provide a summary of the following string. \"\n",
    "                f\"The summary should be concise and informative: {content}. \"\n",
    "            )\n",
    "        case \"zh\":\n",
    "            prompt = (\n",
    "                f\"请提供以下字符串的摘要。\"\n",
    "                f\"摘要应简明扼要且信息丰富: {content}. \"\n",
    "            )\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported language: {lang}\")\n",
    "    \n",
    "    message = {\n",
    "        \"role\": role,\n",
    "        \"content\": prompt,\n",
    "    }\n",
    "    response =  await client.chat(\n",
    "        model=model,\n",
    "        messages=[message]\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "image_link_pattern = r'[^\\s]+\\.(?:jpg|jpeg|png|gif|bmp|webp)'\n",
    "\n",
    "async def get_image_summary_response_async(client: AsyncClient, image_link: str | Path, model:str=\"gemma3:27b\", role:str=\"user\", lang: str='zh') -> ChatResponse:\n",
    "    \"\"\"\n",
    "    Returns a ChatResponse from the chat model with a summary prompt for the given image.\n",
    "    \"\"\"\n",
    "    if isinstance(image_link, Path):\n",
    "        image_link = str(image_link)\n",
    "    # if not re.match(image_link_pattern, image_link):\n",
    "    #     # If the image link is not a URL, throw an error\n",
    "    #     raise ValueError(f\"Invalid image link: {image_link}\")\n",
    "    \n",
    "    match lang:\n",
    "        case 'en':\n",
    "            prompt = \"Please provide a summary of the following image. The summary should be concise and informative about the robot.\"\n",
    "        case 'zh':\n",
    "            prompt = \"请提供以下图像的摘要。关于机器人机械尺寸,运动范围,自由度的说明应简明扼要。\"\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "    response = await client.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": role,\n",
    "                \"content\": prompt,\n",
    "                'images': [f\"{image_link}\"],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f34b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def get_text_summary_response_openai_async(client: AsyncOpenAI, content: str, model:str=\"qwen-plus-2025-07-28\", role:str=\"user\", lang: str=\"zh\") -> str:\n",
    "    \"\"\"\n",
    "    Returns text summary using AsyncOpenAI client for Dashscope integration.\n",
    "    Uses AsyncOpenAI client instead of Ollama AsyncClient.\n",
    "    \"\"\"\n",
    "    match lang:\n",
    "        case \"en\":\n",
    "            prompt = (\n",
    "                f\"Please provide a summary of the following string. \"\n",
    "                f\"The summary should be concise and informative: {content}. \"\n",
    "            )\n",
    "        case \"zh\":\n",
    "            content = re.sub(r\"\\s+\", \" \", content.strip())\n",
    "            prompt = (\n",
    "                f\"请提供以下字符串的摘要。\"\n",
    "                f\"摘要应简明扼要且信息丰富: {content}. \"\n",
    "            )\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported language: {lang}\")\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": role, \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def get_image_summary_response_openai_async(client: AsyncOpenAI, image_link: str | Path, model:str=\"qwen-vl-plus-2025-01-26\", role:str=\"user\", lang: str='zh') -> str:\n",
    "    \"\"\"\n",
    "    Returns image summary using AsyncOpenAI client for Dashscope integration.\n",
    "    Uses AsyncOpenAI client with base64 encoding instead of Ollama AsyncClient.\n",
    "    \"\"\"\n",
    "    if isinstance(image_link, Path):\n",
    "        image_link = str(image_link)\n",
    "    if not re.search(image_link_pattern, image_link):\n",
    "        raise ValueError(f\"Invalid image link: {image_link}\")\n",
    "    \n",
    "    # Read and encode image as base64\n",
    "    import base64\n",
    "    with open(image_link, \"rb\") as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    match lang:\n",
    "        case 'en':\n",
    "            prompt = \"Please provide a summary of the following image. The summary should be concise and informative about the robot.\"\n",
    "        case 'zh':\n",
    "            prompt = \"请提供以下图像的摘要。摘要应简明扼要，信息丰富，描述所显示的机器人内容。\"\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported language: {lang}\")\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\n",
    "            \"role\": role,\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "            ]\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def get_summary_response_async(client: AsyncClient | AsyncOpenAI, content: str, model:str=\"gemma3:27b\", role:str=\"user\", lang: str=\"zh\") -> str:\n",
    "    \"\"\"\n",
    "    Unified async function that routes to the appropriate client-specific function.\n",
    "    - AsyncClient (Ollama): uses get_text_summary_response_async\n",
    "    - AsyncOpenAI: uses get_text_summary_response_openai_async\n",
    "    \"\"\"\n",
    "    if isinstance(client, AsyncClient):\n",
    "        # Ollama client\n",
    "        response = await get_text_summary_response_async(client, content, model, role, lang)\n",
    "        return response.message.content if hasattr(response, 'message') and hasattr(response.message, 'content') else \"\"\n",
    "    elif isinstance(client, AsyncOpenAI):\n",
    "        # OpenAI client\n",
    "        return await get_text_summary_response_openai_async(client, content, model, role, lang)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported client type: {type(client)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9eb566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "async def get_image_summary_async(client: AsyncClient | AsyncOpenAI, image_link: str | Path, model:str=\"gemma3:27b\", role:str=\"user\", lang: str='zh') -> str:\n",
    "    \"\"\"\n",
    "    Unified async function for image analysis that routes to the appropriate client-specific function.\n",
    "    - AsyncClient (Ollama): uses get_image_summary_response_async\n",
    "    - AsyncOpenAI: uses get_image_summary_response_openai_async\n",
    "    \"\"\"\n",
    "    if isinstance(client, AsyncClient):\n",
    "        # Ollama client\n",
    "        response = await get_image_summary_response_async(client, image_link, model, role, lang)\n",
    "        return response.message.content if hasattr(response, 'message') and hasattr(response.message, 'content') else \"\"\n",
    "    elif isinstance(client, AsyncOpenAI):\n",
    "        # OpenAI client  \n",
    "        return await get_image_summary_response_openai_async(client, image_link, model, role, lang)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported client type: {type(client)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac42d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# OpenAI client for text summarization \n",
    "openai_client = AsyncOpenAI(\n",
    "    api_key=os.environ.get('DASHSCOPE_API_KEY'),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "# Ollama client for embeddings and image analysis\n",
    "ollama_client = AsyncClient()\n",
    "\n",
    "# For backward compatibility, keep chat_client pointing to OpenAI client\n",
    "chat_client = openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f06588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# os.getcwd()\n",
    "image_link = \"../res/siasun_md_sammple_hrsl/SN024002/img/__2.png\"\n",
    "# image_link\n",
    "res = re.match(image_link_pattern, image_link)\n",
    "res.group(0)\n",
    "# get_image_summary_response(\"../res/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28596903",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagepath = Path(os.getcwd()).parent / 'res/siasun_md_sample_hrsl/SN024002/img/img_13.png'\n",
    "# imagepath = Path(os.getcwd()).parent / 'res/siasun_md_sample/SN024002/img/img_13.png'\n",
    "# imagepath = Path(os.getcwd()).parent / 'siasun_md_sample_hrsl/SN024002/img/img_11.png'\n",
    "# imagepath = Path(os.getcwd()).parent / 'res/siasun_md_sample_hrsl/SX322002/img/img_13.png'\n",
    "image_link = str(imagepath)\n",
    "image_link\n",
    "display(Markdown(f\"![image]({image_link})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400bfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = re.match(image_link_pattern, 'http://baidu.com/?home/img/small.png')\n",
    "res = re.match(image_link_pattern, 'img/small.png')\n",
    "# res = re.match(image_link_pattern, image_link)\n",
    "# in case of match, print the matched string\n",
    "if res:\n",
    "    print(f\"Matched image link: {res.group(0)}\")\n",
    "else:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    # response = await get_image_summary_response_async(image_link, model=\"gemma3:27b\", role=\"user\", lang='zh')\n",
    "    response = await get_image_summary_response_async(chat_client, image_link, model=\"gemma3:27b\", role=\"user\", lang='zh')\n",
    "    assert isinstance(response.message.content, str), \"Response content should be a string\"\n",
    "    md_text = markdown.markdown(response.message.content)\n",
    "    Markdown(md_text)\n",
    "    # print(md_text)\n",
    "    # print(response.content)\n",
    "    # response_txt = await get_text_summary_response_async(md_text,model=\"gemma3:27b\", role=\"user\", lang='zh')\n",
    "    # md_text = markdown.markdown(response_txt.message.content)\n",
    "    # Markdown(md_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    # response = await get_image_summary_response_async(image_link, model=\"gemma3:27b\", role=\"user\", lang='zh')\n",
    "    response = await get_image_summary_response_async(chat_client, image_link, model=\"gemma3:27b\", role=\"user\", lang='zh')\n",
    "    assert isinstance(response.message.content, str), \"Response content should be a string\"\n",
    "    md_text = markdown.markdown(response.message.content)\n",
    "    Markdown(md_text)\n",
    "    # print(md_text)\n",
    "    # print(response.content)\n",
    "    # response_txt = await get_text_summary_response_async(md_text,model=\"gemma3:27b\", role=\"user\", lang='zh')\n",
    "    # md_text = markdown.markdown(response_txt.message.content)\n",
    "    # Markdown(md_text)gte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee08de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "model = \"deepseek-3.2v:cloud\"  # \"deepseek-r1:32b\" \"gemma3:27b\"\n",
    "if not (\"cloud\" in model):\n",
    "    chat_client: AsyncClient = AsyncClient()   \n",
    "else:\n",
    "    chat_client = AsyncClient(\n",
    "        host=\"http://ollama.com\",\n",
    "        headers={\"Authorization\": f\"Bearer {os.environ.get('OLLAMA_API_KEY')}\"}\n",
    "        )\n",
    "msg_queue: asyncio.Queue = asyncio.Queue(maxsize=8)  # Limit the queue size to 8 messages\n",
    "resp_queue: asyncio.Queue = asyncio.Queue(maxsize=1)  # Limit the queue size to 1 message for response \n",
    "lock = asyncio.Lock()  # Lock to ensure thread-safe access to the queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ccfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "model = \"qwen-plus-2025-07-28\"  #  \"deepseek-v3.2\"  #  \n",
    "chat_client = AsyncOpenAI(\n",
    "    api_key=os.environ.get('DASHSCOPE_API_KEY'),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7130886",
   "metadata": {},
   "source": [
    "## DOM Class with pypandoc Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319435e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ResponseStatus = Enum(\"ResponseStatus\", [\"PENDING\", \"PROCESSING\", \"REORGNIZED\", \"SEMANTICIZED\", \"CANCELLED\", \"COMPLETED\", \"ERROR\"])\n",
    "class AnalysisStatus:\n",
    "    \"\"\"\n",
    "    Enum to represent the status of the analysis process.\n",
    "    \"\"\"\n",
    "    status: ResponseStatus = Field(ResponseStatus.PENDING, description=\"Current status of the analysis process\")\n",
    "    exception: str = Field(\"\", description=\"Exception message if any error occurs during analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775311ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from platform import node\n",
    "from typing import Iterator, Callable, Optional, ClassVar\n",
    "from queue import LifoQueue\n",
    "import asyncio\n",
    "import re\n",
    "import copy\n",
    "import jsoncfg\n",
    "from jsoncfg.config_classes import ConfigJSONArray, ConfigJSONObject, ConfigJSONScalar, ConfigNode\n",
    "\n",
    "class DOM(BaseModel):\n",
    "    model_config = {\"arbitrary_types_allowed\": True}\n",
    "    \n",
    "    # The content of the Markdown document. This can be a string containing Markdown syntax.\n",
    "    raw_markdown: Optional[str] = Field(None, description=\"Raw Markdown content\")\n",
    "    # raw json\n",
    "    raw_json: Optional[str] = Field(None, description=\"Raw JSON content\")\n",
    "    # The json representation of the Markdown AST.\n",
    "    ast_json: Optional[str] = Field(None, description=\"JSON representation of the Markdown AST\")\n",
    "    ast_json_file: Optional[Path] = Field(None, description=\"Path to the JSON file of the Markdown AST\")\n",
    "    semantics_json: Optional[str] = Field(None, description=\"Semantics JSON representation of the Markdown AST\")\n",
    "    semantics_json_file: Optional[Path] = Field(None, description=\"Path to the JSON file of the Markdown AST semantics\")\n",
    "    embed_json: Optional[str] = Field(None, description=\"Semantics JSON representation with embeddings and meta information of the Markdown AST\")\n",
    "    embed_json_file: Optional[Path] = Field(None, description=\"Path to the JSON file of the Markdown AST embeddings\")\n",
    "    file_path: Optional[Path] = Field(None, description=\"Path to the Markdown file\")\n",
    "    root_path: Path = Field(default_factory=Path, description=\"root path of the markdown document, required to get access to the images\")\n",
    "    table_count: int = Field(0, description=\"Number of tables in the Markdown document\")\n",
    "    section_count: int = Field(0, description=\"Number of headers in the Markdown document\")\n",
    "    section_level: list = Field(default_factory=list, description=\"List of section levels in the Markdown document\")\n",
    "    title:str = Field('', description=\"Title of the Markdown document, if available\")\n",
    "    ollama_client: AsyncClient = Field(default_factory=AsyncClient, description=\"AsyncClient instance for embeddings and image analysis (Ollama)\")\n",
    "    openai_client: AsyncOpenAI = Field(default_factory=lambda: AsyncOpenAI(api_key=os.environ.get('DASHSCOPE_API_KEY'), base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"), description=\"AsyncOpenAI instance for text summarization (OpenAI-compatible APIs)\")\n",
    "    db_client: ClientAPI = Field(default_factory=PersistentClient, description=\"PersistentClient instance for database interactions\")\n",
    "    analysis_status: AnalysisStatus = Field(default_factory=AnalysisStatus, description=\"Analysis status of the Markdown document\")\n",
    "\n",
    "\n",
    "    # Use ClassVar to indicate these are class variables, not instance fields\n",
    "    TextBlock_Types: ClassVar[set[str]] = {\n",
    "        \"Plain\",\n",
    "        \"Para\",\n",
    "        \"Figure\",\n",
    "        \"LineBlock\",\"CodeBlock\",\"RawBlock\",\"OrderedList\",\"BulletList\",\"DefinitionList\",\n",
    "        \"Header\",\"BlockQuote\",\n",
    "        \"Table\",\"TableRow\", \"TableCell\"}\n",
    "\n",
    "    NonTextBlock_Types: ClassVar[set[str]] = {\"HorizontalRule\", \"Div\", \"Null\"}\n",
    "\n",
    "    Embed_Types: ClassVar[set[str]] = {\"Section\", \"Image\", \"Table\"}\n",
    "\n",
    "    Block_Types: ClassVar[set[str]] = TextBlock_Types.union(TextBlock_Types)\n",
    "\n",
    "    Inline_Types: ClassVar[set[str]] = {\n",
    "        \"Str\", \"Emph\", \"Strong\", \"Strikeout\", \"Superscript\", \"Subscript\",\n",
    "        \"Decimal\", \"Period\",\n",
    "        \"Link\", \n",
    "        \"Image\", \"Code\", \"Math\", \"RawInline\", \"SoftBreak\", \"HardBreak\", \"Span\"   \n",
    "    }\n",
    "    \n",
    "    Element_Types: ClassVar[set[str]] = Block_Types | Inline_Types\n",
    "    leaf_min_len: ClassVar[int] = 100  # Minimum length of text to consider for summarization\n",
    "\n",
    "    \n",
    "    def __init__(self, md_file_path: Path, ollama_client: Optional[AsyncClient] = None, openai_client: Optional[AsyncOpenAI] = None, db_client: Optional[ClientAPI] = None, **data):\n",
    "        \"\"\" Initializes the Markdown object with raw Markdown content.\n",
    "        If content is provided, it will be set as the raw_markdown.\n",
    "        \"\"\"\n",
    "        # Set defaults for data if not provided\n",
    "        data.setdefault('file_path', Path(md_file_path))\n",
    "        data.setdefault('root_path', Path(md_file_path).parent)\n",
    "        data.setdefault('title', Path(md_file_path).stem)\n",
    "        data.setdefault('ollama_client', ollama_client or AsyncClient())\n",
    "        data.setdefault('openai_client', openai_client or AsyncOpenAI(api_key=os.environ.get('DASHSCOPE_API_KEY'), base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"))\n",
    "        data.setdefault('db_client', db_client or PersistentClient())\n",
    "        data.setdefault('ast_json_file', None)\n",
    "        data.setdefault('embed_json_file', None)\n",
    "        data.setdefault('semantics_json_file', None)\n",
    "\n",
    "        super().__init__(**data)\n",
    "        \n",
    "\n",
    "    def setup(self):\n",
    "        content = self.file_path.read_text(encoding=\"utf-8\")  # type: ignore\n",
    "        if content:\n",
    "\n",
    "            self.raw_markdown = content\n",
    "            self.raw_json = pypandoc.convert_text(self.raw_markdown, \"json\", \"md\")\n",
    "            self.ast_json_file = self.file_path.parent / (str(self.file_path.stem) + \"_ast.json\")  # type: ignore\n",
    "\n",
    "            if self.ast_json_file.exists():\n",
    "                self.ast_json = self.ast_json_file.read_text(encoding=\"utf-8\")  # type: ignore\n",
    "            else:\n",
    "                print(f\"--- IGNORE --- {self.file_path}\")\n",
    "                self.ast_json = None\n",
    "                \n",
    "                slide_splitter = r\"(^<!--\\s*Slide number:\\s*\\d+\\s*-->$)\"  # Regex to match slide splitters in the Markdown content\n",
    "                # If the raw_markdown contains slide splitters, we need to reorganize the slides\n",
    "                if re.search(slide_splitter, self.raw_markdown, flags=(re.MULTILINE|re.IGNORECASE)):  # type: ignore\n",
    "                    # If there are slide splitters, we need to reorganize the slides\n",
    "                    self.ast_json = self.reorg_slides(slide_splitter=slide_splitter)\n",
    "                else:\n",
    "                    # If there are no slide splitters, we can use the raw_json as is\n",
    "                    self.ast_json = self.reorg()\n",
    "                self.ast_json_file.write_text(self.ast_json, encoding=\"utf-8\")  # type: ignore\n",
    "\n",
    "            self.semantics_json_file = self.file_path.parent / (str(self.file_path.stem) + \"_semantics.json\")  # type: ignore\n",
    "            if self.semantics_json_file.exists():\n",
    "                self.semantics_json = self.semantics_json_file.read_text(encoding=\"utf-8\")  # type: ignore\n",
    "\n",
    "            self.embed_json_file = self.file_path.parent / (str(self.file_path.stem) + \"_embed.json\")  # type: ignore\n",
    "            if self.embed_json_file.exists():\n",
    "                self.embed_json = self.embed_json_file.read_text(encoding=\"utf-8\")  # type: ignore\n",
    "        else:\n",
    "            self.ast_json_file = None\n",
    "            self.semantics_json_file = None\n",
    "            self.embed_json_file = None\n",
    "            self.raw_markdown = None\n",
    "            self.raw_json = None\n",
    "            self.ast_json = None\n",
    "\n",
    "    def reorg_slides(self, slide_splitter: str = r\"(^<!--\\s*Slide number:\\s*\\d+\\s*-->$)\") -> str:\n",
    "        \"\"\"Reorganizes the slides in the Markdown AST.\n",
    "        This function splits the raw_markdown into slides based on the slide_splitter regex,\n",
    "        and then reorganizes each slide into a Section with a Header and Content.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.raw_json or self.raw_markdown, \"raw_json/raw_markdown content is empty. Cannot reorganize slides.\"\n",
    "        # Split the raw_markdown into slides\n",
    "        items = re.split(slide_splitter, self.raw_markdown, flags=re.MULTILINE)  # type: ignore\n",
    "        presentation = json.loads(self.raw_json)  # type: ignore\n",
    "        presentation['blocks'] = []  # type: ignore\n",
    "        slide_header0 = {\n",
    "            't': 'Section', \n",
    "            'c': [\n",
    "                {\n",
    "                    't': 'Header', \n",
    "                    'c': [\n",
    "                        1,  # Header level, can be 1, 2, 3, etc.\n",
    "                        ['slide header', [], []],  # The header format list [id, formtat1, format2]\n",
    "                        [{'t': 'Str', 'c': 'slide header'}],  # The header content list\n",
    "                    ]\n",
    "                },  # Header for the slide\n",
    "                {\n",
    "                    't': 'Content', \n",
    "                    'c': []\n",
    "                }\n",
    "                ]  # Content of the slide\n",
    "            }\n",
    "        # Reorganize each slide\n",
    "        slide_header = None  # Initialize slide variable\n",
    "        if items[0] == \"\":\n",
    "            # If the first slide is empty, remove it\n",
    "            items = items[1:]\n",
    "            slide_header = copy.deepcopy(slide_header0)\n",
    "            slide_header['c'][0]['c'][1][0] = '<!-- Slide number: 0 -->'\n",
    "            slide_header['c'][0]['c'][2][0]['c'] = '<!-- Slide number: 0 -->'\n",
    "\n",
    "        for item in items:\n",
    "            if re.match(r\"^<!--\\s*Slide number:\\s*\\d+\\s*-->$\", item):\n",
    "                # If the slide is a slide splitter\n",
    "                slide_header = copy.deepcopy(slide_header0)  # must be deepcopy, otherwise the slide will be modified in place\n",
    "                slide_header['c'][0]['c'][1][0] = item.strip()\n",
    "                slide_header['c'][0]['c'][2][0]['c'] = item.strip()\n",
    "            else:\n",
    "                assert slide_header, f\"Slide is not defined!\"\n",
    "                raw_ast = pypandoc.convert_text(item.strip(), \"json\", \"md\")\n",
    "                ast = json.loads(raw_ast)\n",
    "                assert ast.get('blocks'), f\"AST blocks are not defined in {ast}\"\n",
    "                it = iter(ast['blocks'])\n",
    "                slide, it = self.reorg_section(slide_header, it, bIgnoreLevel=True)         \n",
    "                presentation['blocks'].append(slide)\n",
    "\n",
    "        # Convert the list of slides back to a single JSON object\n",
    "        return json.dumps(presentation, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n",
    "\n",
    "    def to_markdown(self) -> str | None:\n",
    "        return self.raw_markdown\n",
    "\n",
    "    def to_html(self) -> str | None:\n",
    "        if not self.raw_markdown:\n",
    "            return None\n",
    "        # Convert raw Markdown to HTML using pypandoc\n",
    "        return pypandoc.convert_text(self.raw_markdown, \"html\", \"md\")\n",
    "\n",
    "    def to_latex(self) -> str | None:\n",
    "        if not self.raw_markdown:\n",
    "            return None\n",
    "        return pypandoc.convert_text(self.raw_markdown, \"latex\", \"md\")\n",
    "\n",
    "    def to_json(self) -> str | None:\n",
    "        \"\"\" Converts the Markdown content to a JSON representation of its AST.\n",
    "        This uses pypandoc to convert the Markdown content into a JSON format.\n",
    "        \"\"\"\n",
    "        return self.raw_json\n",
    "\n",
    "    async def embed(self, action: Optional[Callable] = None, db_path: Optional[Path] = Path(\"../db\")) -> str:\n",
    "        \"\"\" Walks through the Markdown AST and applies an action to each node.\n",
    "        If no action is provided, it defaults to the identity function.\n",
    "        \"\"\"\n",
    "        if not self.semantics_json:\n",
    "            raise ValueError(\"semantics_json content is empty. Cannot embed the content.\")\n",
    "        if action is None:\n",
    "            action = self.__class__.identity\n",
    "\n",
    "        if self.embed_json:\n",
    "            # If embed_json is already set, we don't need to walk the AST again\n",
    "            print(\"embed_json is already set. Skipping walk.\")\n",
    "            return self.embed_json\n",
    "\n",
    "        ast = json.loads(self.semantics_json)  # type: ignore\n",
    "\n",
    "        assert isinstance(ast, dict), f\"AST should be a dictionary, got {type(ast)}\"\n",
    "        canonical_json = json.dumps(ast, \n",
    "                                    sort_keys=True, \n",
    "                                    ensure_ascii=False,\n",
    "                                    separators=(',', ';')\n",
    "                                    ).encode(\"utf-8\").decode(\"utf-8\")\n",
    "        full_canonical_string = str(self.file_path) + canonical_json\n",
    "        # Create a unique ID for the AST based on its content\n",
    "        ast['i'] = hashlib.sha256(full_canonical_string.encode(\"utf-8\")).hexdigest()\n",
    "        response = await self.ollama_client.embed(model=\"bge-m3\", input=ast.get('summary', ''))  # Initialize the embedding list\n",
    "        ast['e'] = response.embedding  # type: ignore\n",
    "        cur_object_path = [ast['i']]  # Initialize the current object path with the AST ID\n",
    "        ast['m'] = {\n",
    "            'obj_path': cur_object_path,\n",
    "            'embed_model': 'bge-m3',  # type: ignore\n",
    "        }\n",
    "\n",
    "        self.db_client = PersistentClient(path=db_path, settings=Settings(allow_reset=True))  # type: ignore\n",
    "        # Create or get the collection in the database 'mitochondria'\n",
    "        # collection = self.db_client.get_or_create_collection(name=\"mitochondria\")  # defined in the closure for embed_node\n",
    "        # Create or get the 'mitochondria' collection 'mitochondria' in the ephemeral database for testing purposes, to be commented out\n",
    "        ephemeral_db_client = chromadb.EphemeralClient()  # type: ignore     \n",
    "        assert ephemeral_db_client, \"Failed to create ephemeral ChromaDB client.\"\n",
    "            # defined in the closure for embed_node\n",
    "        collection = ephemeral_db_client.get_or_create_collection(name=\"mitochondria\")\n",
    "        assert collection, \"Failed to create or get the 'mitochondria' collection in the database.\"        \n",
    "        collection.add(\n",
    "            ids=[ast['i']],\n",
    "            metadatas=[ast['m']],\n",
    "            embeddings=ast['e'],  # type: ignore\n",
    "            documents=[ast.get('summary', '')]\n",
    "        )\n",
    "\n",
    "        async def embed_node(node: dict) -> dict:\n",
    "            \"\"\"embeds the node summary text\"\"\"\n",
    "            nonlocal collection, cur_object_path  # declare the collection variable in the closure\n",
    "            \n",
    "            assert node.get('t') in self.Embed_Types, f\"Node type {node.get('t')} is not in Embed_Types: {self.Embed_Types}\"\n",
    "            # assert node.get('s'), f\"Node summary text is empty for node: {node}\"\n",
    "            \n",
    "            canonical_string = json.dumps(node, \n",
    "                                        sort_keys=True, \n",
    "                                        ensure_ascii=False,\n",
    "                                        separators=(',',';')\n",
    "                                    ).encode(\"utf-8\").decode(\"utf-8\")\n",
    "            # Create a unique ID for the node based on its content\n",
    "            full_canonical_string = str(cur_object_path) + canonical_string\n",
    "\n",
    "            # Create the identifier for the node\n",
    "            # The identifier is a SHA256 hash of the full canonical string\n",
    "            # This ensures that the identifier is unique and consistent for the same content\n",
    "            node['i'] = hashlib.sha256(full_canonical_string.encode(\"utf-8\")).hexdigest()\n",
    "            # Create the embedding for the node summary\n",
    "            response = await self.ollama_client.embed(model=\"bge-m3\", input=node.get('s', ''))\n",
    "            node['e'] = response.embedding  # type: ignore\n",
    "            # Add meta information\n",
    "            node['m'] = {\n",
    "                'obj_path': cur_object_path,  # type: ignore\n",
    "                'embed_model' : response.model,  # type: ignore\n",
    "            }\n",
    "            collection.add(\n",
    "                ids=[node['i']],\n",
    "                metadatas=[node['m']],\n",
    "                embeddings=node['e'],  # type: ignore\n",
    "                documents=[node.get('summary', '')]\n",
    "            )\n",
    "\n",
    "            return node\n",
    "\n",
    "        async def walk_node(node: dict|list) -> dict|list:\n",
    "            nonlocal cur_object_path\n",
    "\n",
    "            node = action(node)\n",
    "            if isinstance(node, dict):\n",
    "                if node.get('t') in self.Embed_Types:\n",
    "                    node = await embed_node(node)  # Embed the node summary text\n",
    "                    cur_object_path.append(node.get('i', ''))  # Add the node ID to the current object path\n",
    "                for key, value in node.items():\n",
    "                    if isinstance(value, list):\n",
    "                        node[key] = [\n",
    "                            walk_node(child)\n",
    "                            if isinstance(child, (dict, list))\n",
    "                            else child\n",
    "                            for child in value\n",
    "                        ]\n",
    "                    elif isinstance(value, dict):\n",
    "                        if value.get('t') in self.Embed_Types:\n",
    "                            value = await embed_node(value)  # Embed the node summary text\n",
    "                        node[key] = await walk_node(value)\n",
    "            elif isinstance(node, list):\n",
    "                node = [\n",
    "                    await walk_node(child) if isinstance(child, (dict, list)) else child\n",
    "                    for child in node\n",
    "                ]\n",
    "            return node \n",
    "\n",
    "        ast = await walk_node(ast)\n",
    "        self.embed_json = json.dumps(ast, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n",
    "        embed_json_file = self.file_path.parent / (str(self.file_path.stem) + \"_embed.json\")  # type: ignore\n",
    "        with open(embed_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(self.embed_json)\n",
    "        return self.embed_json\n",
    "\n",
    "    def walk_nodes_with_line_number(self, action: Optional[Callable] = None) -> None:\n",
    "        \"\"\" Walks through the Markdown AST and applies an action to each node.\n",
    "        If no action is provided, it defaults to the identity function.\n",
    "        \"\"\"\n",
    "        if not self.raw_json:\n",
    "            raise ValueError(\"raw_json content is empty. Cannot walk the AST.\")\n",
    "        if action is None:\n",
    "            action = self.__class__.identity\n",
    "\n",
    "        ast = jsoncfg.load_config(str(self.ast_json_file))\n",
    "\n",
    "        def walk_node_obj(node)->None:\n",
    "            node = action(node)\n",
    "            if isinstance(node, ConfigJSONObject):\n",
    "                # Dictionary\n",
    "                for key, value in node:\n",
    "                    print(f\"key   \\\"{key}\\\" at line {jsoncfg.node_location(value).line}\")\n",
    "                    walk_node_obj(value)\n",
    "            elif isinstance(node, ConfigJSONArray):\n",
    "                # Array\n",
    "                for item in node:\n",
    "                    walk_node_obj(item)\n",
    "            elif isinstance(node, ConfigJSONScalar):\n",
    "                # Scalar\n",
    "                value = node()\n",
    "                if isinstance(value, str):\n",
    "                    value = value.strip()\n",
    "                print(f\"value \\\"{value}\\\" at line {jsoncfg.node_location(node).line}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown node type: {type(node)}\")\n",
    "\n",
    "        ast = walk_node_obj(ast)\n",
    "    def walk(self, action: Optional[Callable] = None) -> None:\n",
    "        \"\"\" Walks through the Markdown AST and applies an action to each node.\n",
    "        If no action is provided, it defaults to the identity function.\n",
    "        \"\"\"\n",
    "        if not self.raw_json:\n",
    "            raise ValueError(\"raw_json content is empty. Cannot walk the AST.\")\n",
    "        if action is None:\n",
    "            action = self.__class__.identity\n",
    "\n",
    "        ast = json.loads(self.raw_json)\n",
    "\n",
    "        def walk_node(node):\n",
    "            node = action(node)\n",
    "            if isinstance(node, dict):\n",
    "                for key, value in node.items():\n",
    "                    if isinstance(value, list):\n",
    "                        node[key] = [\n",
    "                            walk_node(child)\n",
    "                            if isinstance(child, (dict, list))\n",
    "                            else child\n",
    "                            for child in value\n",
    "                        ]\n",
    "                    elif isinstance(value, dict):\n",
    "                        node[key] = walk_node(value)\n",
    "            elif isinstance(node, list):\n",
    "                node = [\n",
    "                    walk_node(child) if isinstance(child, (dict, list)) else child\n",
    "                    for child in node\n",
    "                ]\n",
    "            return node\n",
    "\n",
    "        ast = walk_node(ast)\n",
    "        self.ast_json = json.dumps(ast, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n",
    "        \n",
    "    def reorg_section(self, section: dict, it: Iterator, bIgnoreLevel:bool=False) -> tuple[dict, Iterator]:\n",
    "        \"\"\"\n",
    "        Reorganizes the list of nodes after a Section node.\n",
    "        Puts all nodes after the Section into the Content of the Section.\n",
    "        This is done until the next Section node is encountered.\n",
    "        Returns the new Section node\n",
    "        \"\"\"\n",
    "\n",
    "        assert section.get('t') == \"Section\", \"Expected a Section node\"\n",
    "        level = section['c'][0]['c'][0]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                item = next(it)\n",
    "                if not bIgnoreLevel and isinstance(item, dict) and item.get('t') == \"Section\":\n",
    "                    # If the item is a Section, add the next items into its Content until the next Section\n",
    "                    next_level = item['c'][0]['c'][0]\n",
    "                    if next_level >= level + 1:  # lower levels need to be continuous\n",
    "                        # If the next Section is at a lower level, step into further subsection handling\n",
    "                        item, it = self.reorg_section(item, it, bIgnoreLevel=False)\n",
    "                    elif next_level == level:  # lower levels need to be continuous\n",
    "                        # If the next Section is at the same level, we can stop here at the current recursion level\n",
    "                        it = iter([item] + list(it))\n",
    "                        return section, it\n",
    "                        # item, it = reorg_section(item, it)\n",
    "                    elif next_level < level:  # higher levels don't need to be continuous\n",
    "                        # If the next Section is at a higher level, we can stop here at the current recursion level\n",
    "                        it = iter([item] + list(it))\n",
    "                        return section, it\n",
    "                    else:  # next_level >= level + 2:\n",
    "                        raise ValueError(f\"Unexpected Section level encountered: node: {item}\")\n",
    "\n",
    "                # If the item is not a Section, we can add it to the Content of the Section\n",
    "                assert section['c'][1].get('t') == \"Content\", \"Expected a Content node\"\n",
    "                section['c'][1]['c'].append(item)\n",
    "\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return section, it\n",
    "        \n",
    "\n",
    "    def reorg(self, action: Optional[Callable] = None) -> str:\n",
    "        \"\"\"\n",
    "        Reorganizes the node structure to ensure that headers are treated as sections.\n",
    "            Applies an action to each node. If no action is provided, it defaults to the identity function.\n",
    "        \"\"\"\n",
    "        if not self.raw_json:\n",
    "            raise ValueError(\"raw_json content is empty. Cannot walk the AST.\")\n",
    "        if action is None:\n",
    "            action = self.__class__.identity\n",
    "\n",
    "        ast = json.loads(self.raw_json)\n",
    "\n",
    "        \n",
    "        def reorg_node(node):\n",
    "            \"\"\"Reorganizes the node structure to ensure that headers are treated as sections.\"\"\"\n",
    "            node = action(node)     \n",
    "\n",
    "            if isinstance(node, dict):\n",
    "                for key, value in node.items():\n",
    "                    if isinstance(value, list):\n",
    "                        new_list = []\n",
    "                        for child in value:\n",
    "                            if isinstance(child, (dict, list)):\n",
    "                                new_list.append(reorg_node(child))\n",
    "                            else:\n",
    "                                new_list.append(child)\n",
    "                            if isinstance(child, dict):\n",
    "                                # If the child is a dict, check if it has a 't' key\n",
    "                                if 't' in child:\n",
    "                                    t = child['t']\n",
    "                                    if t == \"Header\":\n",
    "                                        # If the node is a Header, create a new section \n",
    "                                        # and include the header in its content \n",
    "                                        # along with the following nodes until the next Header\n",
    "                                        child['t'] = \"Section\"\n",
    "                                        child['c'] = [\n",
    "                                            {\n",
    "                                                't': 'Header',\n",
    "                                                'c': [\n",
    "                                                    child['c'][0],  # Header level\n",
    "                                                    child['c'][1],  # Header content\n",
    "                                                    child['c'][2],  # Header content\n",
    "                                                ]\n",
    "                                            },\n",
    "                                            {\n",
    "                                                't': 'Content',\n",
    "                                                'c': []\n",
    "                                            }\n",
    "                                        ]\n",
    "                                        self.section_level.append(child['c'][0]['c'][0])  # Append the header level to section_levell\n",
    "                        it = iter(new_list)\n",
    "                        new_value = []\n",
    "                        while True:\n",
    "                            try:\n",
    "                                item = next(it)\n",
    "                                if isinstance(item, dict) and item.get('t') == \"Section\":  # the first recursion level *\n",
    "                                    item, it = self.reorg_section(item,it, bIgnoreLevel=False)\n",
    "                                new_value.append(item)\n",
    "                            except StopIteration:\n",
    "                                break\n",
    "                        node[key] = new_value\n",
    "                    elif isinstance(value, dict):\n",
    "                        node[key] = reorg_node(value)\n",
    "            elif isinstance(node, list):\n",
    "                node = [\n",
    "                    reorg_node(child) if isinstance(child, (dict, list)) else child\n",
    "                    for child in node\n",
    "                ]\n",
    "            return node\n",
    "\n",
    "        ast = reorg_node(ast)\n",
    "        return json.dumps(ast, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n",
    "        \n",
    "    async def textualize(self, action: Optional[Callable] = None) -> None:\n",
    "        \"\"\" Walks through the Markdown AST and applies an action to each node.\n",
    "        If no action is provided, it defaults to the identity function.\n",
    "        \"\"\"\n",
    "        async def get_leaf_summary_async(node:str, min_len:int) -> str: # If the string length is less than 200, return the node as is\n",
    "            if len(node) < min_len:\n",
    "                return node\n",
    "            # If the node is not an image link, return a summary of the text using unified function\n",
    "            response_content = await get_summary_response_async(self.openai_client, node, model='qwen-plus-2025-07-28', role=\"user\", lang='zh')\n",
    "            if response_content:\n",
    "                return response_content\n",
    "            else:\n",
    "                # If the response is empty, return the original node\n",
    "                return node\n",
    "\n",
    "        async def get_list_summary_async(root: list) -> str:\n",
    "            \"\"\"\n",
    "            Given a list of strings, return a summary of the list.\n",
    "            If the list is empty, return an empty string.\n",
    "            If the list has only one element, return the summary of that element.\n",
    "            If the list has more than one element, return a summary of the concatenated elements.\n",
    "            \"\"\"\n",
    "            list_summary = []\n",
    "            for n in root:\n",
    "                if isinstance(n, str):\n",
    "                    # If the element is a string, get its summary\n",
    "                    summary = await get_leaf_summary_async(n, min_len=self.leaf_min_len)\n",
    "                elif isinstance(n, int) or isinstance(n, float):\n",
    "                    # If the element is a number, get its summary\n",
    "                    summary = await get_leaf_summary_async(str(n), min_len=self.leaf_min_len)\n",
    "                elif isinstance(n, dict):\n",
    "                    # If the dict has a 's' key, use it as the summary\n",
    "                    summary = n.get('s', '')\n",
    "                elif isinstance(n, list):\n",
    "                    # If the element is a list, get its summary\n",
    "                    summary = await get_list_summary_async(n)\n",
    "                elif n is None:\n",
    "                    # If the element is None, skip it\n",
    "                    summary = \"\"\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported element type: {type(n)} in {n}\")\n",
    "                list_summary.append(summary)\n",
    "                \n",
    "            # Concatenate all elements and summarize\n",
    "            # concatenated = \" \".join(list_summary)\n",
    "            # response = get_text_summary_response(concatenated, model=\"gemma3:27b\", role=\"user\", lang='zh')\n",
    "            # if response.message.content:\n",
    "            #     return response.message.content\n",
    "            # else:\n",
    "            #     # If the response is empty, raise an exception\n",
    "            #     raise ValueError(\"Summary response is empty. Please check the input data.\")\n",
    "            return await get_leaf_summary_async(\" \".join(list_summary), min_len=self.leaf_min_len)\n",
    "\n",
    "        async def summary_nodewline_main(action: Optional[Callable] = None) -> None:\n",
    "            \"\"\"\n",
    "            Main function to walk the AST and summarize nodes.\n",
    "            This function will be called by the walk method.\n",
    "            \"\"\"\n",
    "            if not self.ast_json:\n",
    "                raise ValueError(\"raw_json content is empty. Cannot walk the AST and summarize.\")\n",
    "            if action is None:\n",
    "                action = self.__class__.identity\n",
    "\n",
    "            config_ast = jsoncfg.load_config(str(self.ast_json_file))\n",
    "            ast_dict = json.loads(self.ast_json)\n",
    "            assert isinstance(config_ast, ConfigJSONObject), f\"AST should be a dictionary, got {type(config_ast)}\"\n",
    "            config_blocks = config_ast['blocks']\n",
    "            blocks = ast_dict['blocks']\n",
    "\n",
    "            _ , blocks = await summary_node_pair_async(config_blocks, blocks)\n",
    "            ast_dict['blocks'] = blocks\n",
    "            if ast_dict.get('title') is None:\n",
    "                # If the title is not set, use the file name as the title\n",
    "                if self.title is None:\n",
    "                    self.title = str(self.root_path)\n",
    "            ast_dict['title'] = self.title  # Add the title to the AST\n",
    "            ast_dict['file_path'] = str(self.file_path)  # Add the file path to the AST\n",
    "\n",
    "            assert isinstance(blocks, list), f\"Expected a list of blocks, got {type(blocks)}\"\n",
    "            dict_summary = [b['s'] for b in blocks if isinstance(b, dict) and 's' in b]\n",
    "            dict_summary = [ast_dict['title']] + dict_summary  # Add the title to the summary list\n",
    "            # the summary of the document from the summaries in the list of blocks\n",
    "            if not ast_dict['blocks'] or dict_summary == []:\n",
    "                # If the blocks are empty, set the summary to an empty string\n",
    "                ast_dict['summary'] = \"\"\n",
    "                # If the summary is empty, set the AST JSON to an empty string\n",
    "                self.ast_json = json.dumps(ast_dict, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n",
    "            else:\n",
    "                # If the blocks are not empty, set the summary to the concatenated summaries\n",
    "                doc_summary = await get_leaf_summary_async(\" \".join(dict_summary), min_len=self.leaf_min_len)\n",
    "                ast_dict['summary'] = doc_summary\n",
    "                # Convert the summarized AST back to JSON\n",
    "                self.ast_json = json.dumps(ast_dict, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n",
    "\n",
    "        async def summary_node_pair_async(config_node: ConfigNode, \n",
    "                                        node: dict | list | str | int | float | bool | None) \\\n",
    "                                        -> tuple[ConfigNode, dict | list | str | int | float | bool | None]:\n",
    "            '''\n",
    "            Given a string node, add key,value pair: node['s'] = node_summary, and return the node \n",
    "            '''\n",
    "            if isinstance(node, dict):\n",
    "                assert isinstance(config_node, ConfigJSONObject), f\"Expected ConfigJSONObject, got {type(config_node)}\\n\" \\\n",
    "                    f\"config_node at line: {jsoncfg.node_location(config_node).line}\"\n",
    "                try:\n",
    "                    t = node[\"t\"]\n",
    "                except KeyError:\n",
    "                    raise ValueError(f\"Node does not have a 't' key: {node}\")\n",
    "                if t == \"Image\":  # Image summary, the Image node is as defined in the pandoc AST\n",
    "                    summary = []\n",
    "                    # Get the image caption\n",
    "                    if isinstance(node['c'][1], list):\n",
    "                        for item in node['c'][1]:\n",
    "                            assert isinstance(item, dict), f\"Expected dict in image caption list, got {type(item)} in {item}\\n\" \\\n",
    "                                                f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                                                f\"On line: {jsoncfg.node_location(config_node).line}\"\n",
    "                            # If the caption item is a nested structure, extract the text content\n",
    "                            if isinstance(item.get('c'), str):\n",
    "                                # summary.append(item['c'].strip())\n",
    "                                pass\n",
    "                            else:\n",
    "                                assert isinstance(item.get('c'), list), f\"Expected list in image caption item, got {type(item.get('c'))} in {item}\\n\" \\\n",
    "                                                f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                                                f\"On line: {jsoncfg.node_location(config_node).line}\"\n",
    "                                assert item.get('c') is not None, f\"Image caption item is empty: {item}\\n\"\n",
    "                                for chd in item.get('c'):  # type ignore\n",
    "                                    if isinstance(chd.get('c'), str):\n",
    "                                        # summary.append(chd['c'].strip())\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        raise ValueError(f\"Allow only one level of caption formatting {type(chd.get('c'))} in {chd}\\n\" \\\n",
    "                                                f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                                                f\"On line: {jsoncfg.node_location(config_node).line}\")\n",
    "                                \n",
    "                    else:\n",
    "                        raise ValueError(f\"Invalid image caption format: {node['c'][1]}\\n\" \\\n",
    "                                        f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                                        f\"On line: {jsoncfg.node_location(config_node).line}\")\n",
    "                    # Get the image link and summarize the image\n",
    "                    try:\n",
    "                        # summary.append(node['c'][0])  # The first element are defined to be attributes of the image rendering, i.e. content-irrelevant.\n",
    "                        # If the node is an image, get its link\n",
    "                        image_link = self.root_path / node['c'][2][0]  # Assuming the image link is in the third element of the list\n",
    "                        image_link = str(image_link)\n",
    "                        if image_link and re.search(image_link_pattern, image_link):  # re.match leads to empty match if there's spaces in the path!\n",
    "                            # If the node is an image link, get its summary using unified function\n",
    "                            response_content = await get_image_summary_async(\n",
    "                                client=self.ollama_client,\n",
    "                                image_link=image_link,\n",
    "                                model=\"gemma3:27b\",\n",
    "                                role=\"user\",\n",
    "                                lang='zh'\n",
    "                                )\n",
    "                            # summary.append(response.message.content)\n",
    "                            node[\"s\"] = response_content\n",
    "                        else:\n",
    "                            # If the node is not an image link, summarize its content\n",
    "                            raise ValueError(f\"Invalid image link: {image_link}\")\n",
    "                        # Get the summary of the image caption\n",
    "\n",
    "                    except (IndexError, KeyError):\n",
    "                        # Handle cases where the image link is not in the expected format\n",
    "                        raise ValueError(f\"Invalid image node structure: {node}\\n\" \\\n",
    "                                        f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                                        f\"On line: {jsoncfg.node_location(config_node).line}\")\n",
    "                    # The second element is the image title\n",
    "                    # summary.append(node['c'][2][1])  # The second element is the image title\n",
    "\n",
    "                    # try:\n",
    "                    #     response_txt = await get_text_summary_response_async(\n",
    "                    #         client=self.llm_client,\n",
    "                    #         content=\" \".join(summary),\n",
    "                    #         model=\"gemma3:27b\",\n",
    "                    #         role=\"user\",\n",
    "                    #         lang='zh'\n",
    "                    #     )\n",
    "                    #     node[\"s\"] = response_txt.message.content\n",
    "                    # except Exception as e:\n",
    "                    #     raise ValueError(f\"Error summarizing image node: {node}\\n\" \\\n",
    "                    #                     f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                    #                     f\"On line: {jsoncfg.node_location(config_node).line}\\n\" \\\n",
    "                    #                     f\"Error: {e}\")\n",
    "\n",
    "                    print(f\"Summarize image: {image_link} on line {jsoncfg.node_location(config_node).line}\")\n",
    "                elif t == \"Cite\" or t == \"AlignDefault\" or t == \"ColWidth\" :  # Quoted node will be ignored\n",
    "                    node[\"s\"] = \"\"  # Set the summary to an empty string\n",
    "                else: # TextBlock summary\n",
    "                    c = node.get(\"c\", None)\n",
    "                    if not c:\n",
    "                        node[\"s\"] = \"\"\n",
    "                    else:\n",
    "                        dict_summary = []\n",
    "                        for (ckey,cvalue), (key, value) in zip(config_node, node.items()): # get summary of the values (content)\n",
    "                            if key == \"t\":\n",
    "                                continue\n",
    "                            if isinstance(cvalue, ConfigJSONArray):  # get the summary of the string list\n",
    "                                assert isinstance(value, list), f\"Expected a list for key {key}, got {type(value)} in {value}\\n\" \\\n",
    "                                    f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                                    f\"key {ckey} On line: {jsoncfg.node_location(cvalue).line}\"\n",
    "                                if value == [] or cvalue is None:\n",
    "                                    # If the list is empty, skip it\n",
    "                                    continue\n",
    "                                # If the value is a list, summarize each element\n",
    "                                node[key] = []\n",
    "                                # config_node = []\n",
    "                                for citem, item in zip(cvalue, value):\n",
    "                                    try:\n",
    "                                        _, child = await summary_node_pair_async(citem, item)\n",
    "                                        node[key].append(child)\n",
    "                                    except Exception as e:\n",
    "                                        raise ValueError(f\"Error summarizing list item: {item}\\n\" \\\n",
    "                                                        f\"ConfigNode: {citem} of type {type(citem)}.\\n\" \\\n",
    "                                                        f\"On line: {jsoncfg.node_location(citem).line}\\n\" \\\n",
    "                                                        f\"Error: {e}\")\n",
    "                                    # config_node.append(config_child)\n",
    "                                dict_summary.append(await get_list_summary_async(value))\n",
    "                            elif isinstance(cvalue, ConfigJSONObject):\n",
    "                                assert isinstance(value, dict), f\"Expected a dict for key {key}, got {type(value)} in {value}\\n\" \\\n",
    "                                    f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                                    f\"key {ckey} On line: {jsoncfg.node_location(cvalue).line}\"\n",
    "                                try:\n",
    "                                    config_child, child = await summary_node_pair_async(cvalue, value)  # insert the value['s']\n",
    "                                except Exception as e:\n",
    "                                    raise ValueError(f\"Error summarizing dict item: {value}\\n\" \\\n",
    "                                                    f\"ConfigNode: {cvalue} of type {type(cvalue)}.\\n\" \\\n",
    "                                                    f\"On line: {jsoncfg.node_location(cvalue).line}\\n\" \\\n",
    "                                                    f\"Error: {e}\")\n",
    "                                assert isinstance(child,dict) and 's' in child, f\"Expected dict with 's' key, got {child}\\n\" \\\n",
    "                                    f\"ConfigNode: {config_child} of type {type(config_child)}.\\n\" \\\n",
    "                                    f\"On line: {jsoncfg.node_location(config_child).line}\"\n",
    "                                dict_summary.append(child['s'])\n",
    "\n",
    "                            elif value is None or value == \"\":\n",
    "                                # If the value is None, skip it\n",
    "                                continue\n",
    "                            elif isinstance(cvalue, ConfigJSONScalar):\n",
    "                                assert isinstance(value, (str, int, float)), f\"Expected a scalar for key {key}, got {type(value)} in {value}\\n\" \\\n",
    "                                    f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                                    f\"key {ckey} On line: {jsoncfg.node_location(cvalue).line}\"\n",
    "                                # If the value is a scalar, summarize it\n",
    "                                if isinstance(value, str):\n",
    "                                    value = value.strip()\n",
    "                                dict_summary.append(await get_leaf_summary_async(str(value),min_len=self.leaf_min_len))\n",
    "                        # get the summary of the node\n",
    "                        if dict_summary:  # type: ignore\n",
    "                            # If there are summaries, concatenate them\n",
    "                            node[\"s\"] = await get_leaf_summary_async(\" \".join(dict_summary),min_len=self.leaf_min_len)\n",
    "                            # node[\"s\"] = get_text_summary_response(\n",
    "                            #     \" \".join(dict_summary), model=\"gemma3:27b\", role=\"user\", lang='zh'\n",
    "                            # ).message.content\n",
    "                        else:\n",
    "                            # If no summaries, set to empty string\n",
    "                            node[\"s\"] = \"\"\n",
    "\n",
    "                    # if t is table\n",
    "                    if t == \"Table\":\n",
    "                        print(f\"{self.file_path} Summarize table: {self.table_count} on line {jsoncfg.node_location(config_node).line}\")\n",
    "                        self.table_count += 1\n",
    "                    elif t == \"Section\":\n",
    "                        print(f\"{self.file_path} Summarize section: {self.section_count} on line {jsoncfg.node_location(config_node).line}\")\n",
    "                        self.section_count += 1\n",
    "\n",
    "            elif isinstance(node, list):\n",
    "                assert isinstance(config_node, ConfigJSONArray), f\"Expected ConfigJSONArray, got {type(config_node)}\" \\\n",
    "                    f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                    f\"On line: {jsoncfg.node_location(config_node).line}\"\n",
    "                new_node = []\n",
    "                for citem, item in zip(config_node, node):\n",
    "                    if isinstance(item, (str, int, float, bool)):\n",
    "                        child = str(item).strip()\n",
    "                    elif isinstance(item, (dict, list)):\n",
    "                        try:\n",
    "                            _, child = await summary_node_pair_async(citem, item)\n",
    "                        except Exception as e:\n",
    "                            raise ValueError(f\"Error summarizing list item: {item}\\n\" \\\n",
    "                                            f\"ConfigNode: {citem} of type {type(citem)}.\\n\" \\\n",
    "                                            f\"On line: {jsoncfg.node_location(citem).line}\\n\" \\\n",
    "                                            f\"Error: {e}\")\n",
    "                    # config_node.append(config_child)\n",
    "                    else: # item is None\n",
    "                        child = item\n",
    "                    new_node.append(child)\n",
    "                node = new_node\n",
    "            elif isinstance(node, (str, int, float, bool)):\n",
    "                assert isinstance(config_node, ConfigJSONScalar), f\"Expected ConfigJSONScalar, got {type(config_node)}\" \\\n",
    "                    f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                    f\"On line: {jsoncfg.node_location(config_node).line}\"\n",
    "                try:\n",
    "                    node_summary = await get_leaf_summary_async(str(node), min_len=self.leaf_min_len)\n",
    "                    node = node_summary\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Error summarizing node: {node}\\n\" \\\n",
    "                                    f\"ConfigNode: {config_node} of type {type(config_node)}.\\n\" \\\n",
    "                                    f\"On line: {jsoncfg.node_location(config_node).line}\\n\" \\\n",
    "                                    f\"Error: {e}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported node type: {type(node)} in {node}\\n\" \\\n",
    "                                f\"Unsupported ConfigNode: {config_node} of type {type(config_node)}.\" \\\n",
    "                                f\"On line: {jsoncfg.node_location(config_node).line}\")  \n",
    "\n",
    "            return config_node, node\n",
    "\n",
    "        async def summary_node_main(action: Optional[Callable] = None) -> None:\n",
    "            \"\"\"\n",
    "            Main function to walk the AST and summarize nodes.\n",
    "            This function will be called by the walk method.\n",
    "            \"\"\"\n",
    "            if not self.ast_json:\n",
    "                raise ValueError(\"raw_json content is empty. Cannot walk the AST and summarize.\")\n",
    "            if action is None:\n",
    "                action = self.__class__.identity\n",
    "\n",
    "            ast = json.loads(self.ast_json)\n",
    "            blocks = ast.get(\"blocks\", [])\n",
    "\n",
    "            ast['blocks'] = await summary_node_async(blocks)\n",
    "            if ast.get('title') is None:\n",
    "                # If the title is not set, use the file name as the title\n",
    "                if self.title is None:\n",
    "                    self.title = str(self.root_path)\n",
    "            ast['title'] = self.title  # Add the title to the AST\n",
    "            ast['file_path'] = str(self.file_path)  # Add the file path to the AST\n",
    "\n",
    "            assert isinstance(blocks, list), f\"Expected a list of blocks, got {type(blocks)}\"\n",
    "            dict_summary = [b['s'] for b in blocks if isinstance(b, dict) and 's' in b]\n",
    "            dict_summary = [ast['title']] + dict_summary  # Add the title to the summary list\n",
    "            # the summary of the document from the summaries in the list of blocks\n",
    "            if not ast['blocks'] or dict_summary == []:\n",
    "                # If the blocks are empty, set the summary to an empty string\n",
    "                ast['summary'] = \"\"\n",
    "                # If the summary is empty, set the AST JSON to an empty string\n",
    "                self.ast_json = json.dumps(ast, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n",
    "            else:\n",
    "                # If the blocks are not empty, set the summary to the concatenated summaries\n",
    "                doc_summary = await get_leaf_summary_async(\" \".join(dict_summary), min_len=self.leaf_min_len)\n",
    "                ast['summary'] = doc_summary\n",
    "                # Convert the summarized AST back to JSON\n",
    "                self.ast_json = json.dumps(ast, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n",
    "            \n",
    "        async def summary_node_async(node: dict | list) -> dict | list:\n",
    "            '''\n",
    "            Given a string node, add key,value pair: node['s'] = node_summary, and return the node \n",
    "            '''\n",
    "            if isinstance(node, dict):\n",
    "                try:\n",
    "                    t = node[\"t\"]\n",
    "                except KeyError:\n",
    "                    raise ValueError(f\"Node does not have a 't' key: {node}\")\n",
    "                if t == \"Image\":  # Image summary, the Image node is as defined in the pandoc AST\n",
    "                    summary = []\n",
    "                    if (not node['c'][1] == []) and (node['c'][1][0] is not None) and (node['c'][1][0].get('c') is not None):\n",
    "                        summary.append(node['c'][1][0]['c'])  # The content of the second element is the image caption\n",
    "                    try:\n",
    "                        # summary.append(node['c'][0])  # The first element are defined to be attributes of the image rendering, i.e. content-irrelevant.\n",
    "                        # If the node is an image, get its link\n",
    "                        image_link = self.modelroot_path / node['c'][2][0]  # Assuming the image link is in the third element of the list\n",
    "                        image_link = str(image_link)\n",
    "                        if image_link and re.search(image_link_pattern, image_link):  # re.match leads to empty match if there's spaces in the path!\n",
    "                            # If the node is an image link, get its summary\n",
    "                            response = await get_image_summary_response_async(\n",
    "                                client=self.ollama_client,\n",
    "                                image_link=image_link,\n",
    "                                model=\"gemma3:27b\",\n",
    "                                role=\"user\",\n",
    "                                lang='zh'\n",
    "                                )\n",
    "                            summary.append(response.message.content)\n",
    "                        else:\n",
    "                            # If the node is not an image link, summarize its content\n",
    "                            raise ValueError(f\"Invalid image link: {image_link}\")\n",
    "                        # Get the summary of the image caption\n",
    "\n",
    "                    except (IndexError, KeyError):\n",
    "                        # Handle cases where the image link is not in the expected format\n",
    "                        raise ValueError(f\"Invalid image node structure: {node}\")\n",
    "                    # The second element is the image title\n",
    "                    summary.append(node['c'][2][1])  # The second element is the image title\n",
    "\n",
    "                    response_txt = await get_text_summary_response_async(\n",
    "                        client=self.openai_client,\n",
    "                        content=\" \".join(summary),\n",
    "                        model=\"gemma3:27b\",\n",
    "                        role=\"user\",\n",
    "                        lang='zh'\n",
    "                    )\n",
    "                    node[\"s\"] = response_txt.message.content\n",
    "\n",
    "                    print(f\"Summarize image: {image_link}\")\n",
    "                elif t == \"Cite\" or t == \"AlignDefault\" or t == \"ColWidth\" :  # Quoted node will be ignored\n",
    "                    node[\"s\"] = \"\"  # Set the summary to an empty string\n",
    "                else: # TextBlock summary\n",
    "                    c = node.get(\"c\", None)\n",
    "                    if not c:\n",
    "                        node[\"s\"] = \"\"\n",
    "                    else:\n",
    "                        dict_summary = []\n",
    "                        for key, value in node.items(): # get summary of the values (content)\n",
    "                            if key == \"t\":\n",
    "                                continue\n",
    "                            if isinstance(value, list):  # get the summary of the string list\n",
    "                                if value == []:\n",
    "                                    # If the list is empty, skip it\n",
    "                                    continue\n",
    "                                # If the value is a list, summarize each element\n",
    "                                node[key] = [\n",
    "                                    await summary_node_async(child)\n",
    "                                    if isinstance(child, (dict,list))\n",
    "                                    else child\n",
    "                                    for child in value\n",
    "                                ]\n",
    "                                dict_summary.append(await get_list_summary_async(value))\n",
    "                            elif isinstance(value, dict):\n",
    "                                child = await summary_node_async(value)  # insert the value['s']\n",
    "                                assert isinstance(child, dict) and 's' in child, f\"Expected dict with 's' key, got {child}\"\n",
    "                                dict_summary.append(child['s'])\n",
    "\n",
    "                            elif value is None or value == \"\":\n",
    "                                # If the value is None, skip it\n",
    "                                continue\n",
    "                            else:\n",
    "                                if not isinstance(value, str):\n",
    "                                    # If the value is not a string, convert it to a string\n",
    "                                    value = str(value)\n",
    "                                dict_summary.append(await get_leaf_summary_async(value,min_len=self.leaf_min_len))\n",
    "                        # get the summary of the node\n",
    "                        if dict_summary:  # type: ignore\n",
    "                            # If there are summaries, concatenate them\n",
    "                            node[\"s\"] = await get_leaf_summary_async(\" \".join(dict_summary),min_len=self.leaf_min_len)\n",
    "                            # node[\"s\"] = get_text_summary_response(\n",
    "                            #     \" \".join(dict_summary), model=\"gemma3:27b\", role=\"user\", lang='zh'\n",
    "                            # ).message.content\n",
    "                        else:\n",
    "                            # If no summaries, set to empty string\n",
    "                            node[\"s\"] = \"\"\n",
    "\n",
    "                    # if t is table\n",
    "                    if t == \"Table\":\n",
    "                        print(f\"{self.file_path} Summarize table: {self.table_count}\")\n",
    "                        self.table_count += 1\n",
    "                    elif t == \"Section\":\n",
    "                        print(f\"{self.file_path} Summarize section: {self.section_count} section depth {node['c'][0]['c'][0]}\")\n",
    "                        self.section_count += 1\n",
    "\n",
    "            elif isinstance(node, list):\n",
    "                node = [\n",
    "                    await summary_node_async(child) if isinstance(child, (dict,list)) else child\n",
    "                    for child in node\n",
    "                ]\n",
    "            else:\n",
    "                assert isinstance(node, str), f\"Expected a string node, got {type(node)}, on json line\"\n",
    "                # If the node is a string, get its summary\n",
    "                node_summary = await get_leaf_summary_async(node, min_len=self.leaf_min_len)\n",
    "                node = node_summary\n",
    "\n",
    "            return node\n",
    "\n",
    "        # Run the summary_nodewline_main function asynchronously\n",
    "        # asyncio.run(summary_nodewline_main(action))\n",
    "        await summary_nodewline_main(action)\n",
    "        return \n",
    "        \n",
    "    @classmethod\n",
    "    def identity(cls, obj):\n",
    "        \"\"\"Identity function for use in walk.\"\"\"\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cad730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test walking the AST with line numbers\n",
    "if False:\n",
    "    md_file = \"../res/siasun_md_sample/SN024002/SN024002《新松SN7B-7-0.90规格参数》A-1.md\"\n",
    "    print(f\"Using file: {md_file}\")\n",
    "    md_file = Path(md_file)\n",
    "    print(f\"File exists: {md_file.exists()}\")\n",
    "\n",
    "\n",
    "    if md_file.exists():\n",
    "        dom = DOM(md_file, ollama_client=ollama_client, openai_client=openai_client)\n",
    "        dom.setup()\n",
    "        print(f\"Raw JSON length: {len(dom.raw_json) if dom.raw_json else 0}\")\n",
    "        print(f\"AST JSON length: {len(dom.ast_json) if dom.ast_json else 0}\")\n",
    "        dom.walk_nodes_with_line_number()\n",
    "        # jsoncfg.load_config(str(dom.ast_json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    md_file = Path(os.getcwd()).parent / r'res/md.json.semantics/06 产品手册/软件类/手册/V5.0版本/SX322001《新松机器人通用操作手册》(B-5)/SX322001《新松机器人通用操作手册》(B-5).md'\n",
    "    # md_file = Path(os.getcwd()).parent / r\"res/md.mid.hrsl.test/06 产品手册/机械类/SR122001Installation and Maintenance Manual of Siasun SR12 Series Industrial Robot-A.0_20250304110301/SR122001Installation and Maintenance Manual of Siasun SR12 Series Industrial Robot-A.0_20250304110301.md\" \n",
    "    # md_file = Path(\"/v/data/新型机器人智能问答系统数据源-md/.md.mid.hrsl.test.not.processed/DSS_MD_CoC_25_HEL_014 Issue 1/DSS_MD_CoC_25_HEL_014 Issue 1.md\") \n",
    "    # md_file = Path('/d/devel/rag/ribosome/res/md.hrsl/01 设计标准/SX047001新松机器人产品识别设计标准A-1/SX047001新松机器人产品识别设计标准A-1.md') \n",
    "    \n",
    "    # md_file = Path(os.getcwd()).parent / 'res/siasun_md_sample_hrsl/SR02400401/SR02400401《 SIASUN SR210A-210-2.65 Specifications-CE》A-0.md'\n",
    "    # md_file = Path(os.getcwd()).parent / 'res/siasun_md_sample_hrsl/SX322002/SX322002.md'\n",
    "    # md_file = Path(os.getcwd()).parent / 'res/siasun_md_sample_hrsl/SR024011/SR024011.md'\n",
    "    # md_file = Path(os.getcwd()).parent / 'res/siasun_md_sample_hrsl/SN024002/SN024002.md'\n",
    "    # md_file = Path(os.getcwd()).parent / 'res/siasun_md_sample/SN024002/SN024002.md'\n",
    "    print(f\"Using file: {md_file}\")\n",
    "    print(f\"File exists: {md_file.exists()}\")\n",
    "    \n",
    "    if md_file.exists():\n",
    "        dom = DOM(md_file, ollama_client=ollama_client, openai_client=openai_client)\n",
    "        dom.setup()\n",
    "        print(f\"Raw JSON length: {len(dom.raw_json) if dom.raw_json else 0}\")\n",
    "        print(f\"AST JSON length: {len(dom.ast_json) if dom.ast_json else 0}\")\n",
    "        \n",
    "        js_sections_file = md_file.parent / (str(md_file.stem) + \"_ast.json\")\n",
    "        print(f\"Saving to: {js_sections_file}\")\n",
    "        # Write the JSON representation of the AST to the file\n",
    "        js_sections_file.write_text(dom.ast_json, encoding=\"utf-8\")\n",
    "        \n",
    "        # Test textualize functionality\n",
    "        print(\"Starting textualization...\")\n",
    "        await dom.textualize()\n",
    "        \n",
    "        js_semantics_file = md_file.parent / (str(md_file.stem) + \"_semantics.json\")\n",
    "        js_semantics_file.write_text(dom.ast_json, encoding=\"utf-8\")\n",
    "        print(f\"Semantics saved to: {js_semantics_file}\")\n",
    "    else:\n",
    "        print(\"File does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c93d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_reorg(root_folder: Path | str) -> None:\n",
    "    \"\"\"\n",
    "    iterates through a root folder recursively and analyzes the semantics of each Markdown document.\n",
    "    generate a json file containing the semantical summary of each document \n",
    "    output in the same folder as the original markdown file.capitalize\n",
    "    \"\"\"\n",
    "    root = Path(root_folder) if isinstance(root_folder, str) else root_folder\n",
    "    for file in root.rglob(\"*.md\"):\n",
    "        dom = DOM(file, ollama_client=ollama_client, openai_client=openai_client)\n",
    "        dom.setup()  # Load the Markdown content and convert it to JSON AST\n",
    "        ast_json_file = file.parent / (str(file.stem) + \"_ast.json\")\n",
    "        if not ast_json_file.exists() and dom.ast_json:\n",
    "            ast_json_file.write_text(dom.ast_json, encoding=\"utf-8\")\n",
    "            print(f\"Processing file: {file}\")\n",
    "        elif ast_json_file.exists():\n",
    "            # print(f\"File already processed: {file}\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"AST JSON missing: {file}\")\n",
    "\n",
    "# document_reorg(Path(\"/v/data/新型机器人智能问答系统数据源-md/.md.mid.hrsl.test.not.processed\"))\n",
    "\n",
    "# document_reorg(Path(\"../res/md.mid.hrsl.test\"))\n",
    "# document_reorg(Path(\"/v/data/documents-semantics/.md\"))\n",
    "# document_reorg(Path(\"../res/SN024002\"))\n",
    "# document_reorg(Path(\"../res/test_batch_async\"))\n",
    "# document_reorg(Path(\"/v/data/documents-semantics/.md.hrsl\"))\n",
    "# document_reorg(Path(\"../res/siasun_md_sample_hrsl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def analyze_one_document_async(md_file: Path, semaphore: asyncio.Semaphore) -> DOM:\n",
    "    \"\"\"\n",
    "    Asynchronously analyzes the semantics of a Markdown document.\n",
    "    Returns a DOM object containing the semantical summary of the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with semaphore:  # Limit concurrent access to the semaphore\n",
    "            print(f\"Analyzing document: {md_file}\")\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\")\n",
    "                dom = DOM(md_file, ollama_client=ollama_client, openai_client=openai_client)\n",
    "                dom.setup()  # Load the Markdown content and convert it to JSON AST\n",
    "                if w:\n",
    "                    print(f\"Warnings encountered while setting up DOM for {md_file}: {w}\")\n",
    "            if dom.semantics_json:\n",
    "                dom.analysis_status.status = ResponseStatus.COMPLETED\n",
    "                dom.analysis_status.exception = f\"Document already analyzed: {md_file.stem}\"\n",
    "                return dom\n",
    "\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\")\n",
    "                await dom.textualize()  # Summarize the document\n",
    "                if w:\n",
    "                    print(f\"Warnings encountered while textualizing DOM for {md_file}: {w}\")\n",
    "            if dom.file_path:\n",
    "                semantics_json_file = md_file.parent / (str(md_file.stem) + \"_semantics.json\")\n",
    "                semantics_json_file.write_text(dom.ast_json, encoding=\"utf-8\")  # type: ignore\n",
    "                dom.analysis_status.status = ResponseStatus.COMPLETED  # type: ignore\n",
    "                dom.analysis_status.exception = f\"Finished analyzing document: {md_file.stem}\"\n",
    "    except asyncio.CancelledError:\n",
    "        dom.analysis_status.status = ResponseStatus.CANCELLED  # type: ignore\n",
    "        dom.analysis_status.exception = f\"Analysis cancelled for document: {md_file}\"  # type: ignore\n",
    "    except Exception as e:\n",
    "        dom.analysis_status.status = ResponseStatus.ERROR  # type: ignore\n",
    "        dom.analysis_status.exception = f\"Error occurred while analyzing document: {md_file}, Error: {e}\"  # type: ignore\n",
    "\n",
    "    return dom  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43593949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import embed\n",
    "\n",
    "\n",
    "def check_one_document(md_file: Path, embed: bool = False) -> DOM | None:\n",
    "    \"\"\"\n",
    "    Asynchronously analyzes the semantics of a Markdown document.\n",
    "    Returns a DOM object containing the semantical summary of the document.\n",
    "    \"\"\"\n",
    "    print(f\"Checking document: {md_file}\")\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"always\")\n",
    "        dom = DOM(md_file, ollama_client=ollama_client, openai_client=openai_client)\n",
    "        dom.setup()  # Load the Markdown content and convert it to JSON AST\n",
    "        if w:\n",
    "            print(f\"Warnings encountered while setting up DOM for {md_file}: {w}\")\n",
    "\n",
    "    if embed:\n",
    "        return dom if dom.semantics_json else None\n",
    "    else:\n",
    "        return None if dom.semantics_json else dom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "async def document_semantics_analysis(root_folder: Path) -> None:\n",
    "    \"\"\"\n",
    "    iterates through a root folder recursively and analyzes the semantics of each Markdown document.\n",
    "    generate a json file containing the semantical summary of each document \n",
    "    output in the same folder as the original markdown file.capitalize\n",
    "    \"\"\"\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(4)  # Limit the number of concurrent tasks\n",
    "    # Iterate through all Markdown files in the root folder recursively\n",
    "    # and create a task for each file to process it asynchronously\n",
    "    # files = list(root_folder.rglob(\"*.md\"))\n",
    "    # to_do = [analyze_one_document_async(file, semaphore) for file in files]\n",
    "\n",
    "    doms_to_analyze = [check_one_document(file, embed=False) for file in root_folder.rglob(\"*.md\")]\n",
    "    to_do = [analyze_one_document_async(dom.file_path, semaphore) for dom in doms_to_analyze if dom is not None]  # type: ignore\n",
    "    to_do_iter = asyncio.as_completed(to_do)  # Create an iterator for the tasks\n",
    "    to_do_iter = tqdm.tqdm(to_do_iter, total=len(to_do), desc=\"Processing files\", unit=\"file\")\n",
    "    for coro in to_do_iter:\n",
    "        # Wait for each task to complete and get the result\n",
    "        try:\n",
    "            dom = await coro  # Await the completion of the task\n",
    "        except Exception as e:\n",
    "            # print(f\"Error processing file: {dom.file_path}\")\n",
    "            print(f\"Error processing file: {e}\")\n",
    "            continue\n",
    "        print(f\"Processed file: {dom.file_path} with title: {dom.title}, \"\n",
    "              f\"status: {dom.analysis_status.status}, exception: {dom.analysis_status.exception}\")\n",
    "\n",
    "# Run the document semantics analysis\n",
    "# await document_semantics_analysis(Path(\"../res/md.json.semantics/06 产品手册\"))\n",
    "await document_semantics_analysis(Path(\"../res/md.json.semantics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ecfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def embed_one_document_async(md_file: Path, semaphore: asyncio.Semaphore) -> DOM:\n",
    "    \"\"\"\n",
    "    Asynchronously analyzes the semantics of a Markdown document.\n",
    "    Returns a DOM object containing the semantical summary of the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with semaphore:  # Limit concurrent access to the semaphore\n",
    "            print(f\"Analyzing document: {md_file}\")\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\")\n",
    "                dom = DOM(md_file, ollama_client=ollama_client, openai_client=openai_client)\n",
    "                dom.setup()  # Load the Markdown content and convert it to JSON AST\n",
    "                if w:\n",
    "                    print(f\"Warnings encountered while setting up DOM for {md_file}: {w}\")\n",
    "            if not dom.semantics_json:\n",
    "                dom.analysis_status.status = ResponseStatus.PENDING\n",
    "                dom.analysis_status.exception = f\"Document semantics not found: {md_file.stem}\"\n",
    "                return dom\n",
    "\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\")\n",
    "                await dom.embed()  # Summarize the document\n",
    "                if w:\n",
    "                    print(f\"Warnings encountered while embedding DOM for {md_file}: {w}\")\n",
    "            dom.analysis_status.status = ResponseStatus.COMPLETED  # type: ignore\n",
    "            dom.analysis_status.exception = f\"Finished embedding document: {md_file.stem}\"\n",
    "    except asyncio.CancelledError:\n",
    "        dom.analysis_status.status = ResponseStatus.CANCELLED  # type: ignore\n",
    "        dom.analysis_status.exception = f\"Embedding cancelled for document: {md_file}\"  # type: ignore\n",
    "    except Exception as e:\n",
    "        dom.analysis_status.status = ResponseStatus.ERROR  # type: ignore\n",
    "        dom.analysis_status.exception = f\"Error occurred while embedding document: {md_file}, Error: {e}\"  # type: ignore\n",
    "\n",
    "    return dom  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523358a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "async def document_embedding(root_folder: Path) -> None:\n",
    "    \"\"\"\n",
    "    iterates through a root folder recursively and analyzes the semantics of each Markdown document.\n",
    "    generate a json file containing the semantical summary of each document \n",
    "    output in the same folder as the original markdown file.capitalize\n",
    "    \"\"\"\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(2)  # Limit the number of concurrent tasks\n",
    "    # Iterate through all Markdown files in the root folder recursively\n",
    "    # and create a task for each file to process it asynchronously\n",
    "    # files = list(root_folder.rglob(\"*.md\"))\n",
    "    # to_do = [analyze_one_document_async(file, semaphore) for file in files]\n",
    "\n",
    "    doms_to_embed = [check_one_document(file, embed=True) for file in root_folder.rglob(\"*.md\")]\n",
    "    to_do = [embed_one_document_async(dom.file_path, semaphore) for dom in doms_to_embed if dom is not None]  # type: ignore\n",
    "    to_do_iter = asyncio.as_completed(to_do)  # Create an iterator for the tasks\n",
    "    to_do_iter = tqdm.tqdm(to_do_iter, total=len(to_do), desc=\"Processing files\", unit=\"file\")\n",
    "    for coro in to_do_iter:\n",
    "        # Wait for each task to complete and get the result\n",
    "        try:\n",
    "            dom = await coro  # Await the completion of the task\n",
    "        except Exception as e:\n",
    "            # print(f\"Error processing file: {dom.file_path}\")\n",
    "            print(f\"Error processing file: {e}\")\n",
    "            continue\n",
    "        print(f\"Processed file: {dom.file_path} with title: {dom.title}, \"\n",
    "              f\"status: {dom.analysis_status.status}, exception: {dom.analysis_status.exception}\")\n",
    "\n",
    "# Run the document embedding\n",
    "await document_embedding(Path(\"../res/md.json.semantics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ce06a",
   "metadata": {},
   "source": [
    "## asyncio interface of processing many markdown files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_one(file: Path) -> None:\n",
    "    \"\"\"\n",
    "    Processes a single Markdown file and generates its semantics analysis.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file started: {file}\")\n",
    "    dom = DOM(file, ollama_client=ollama_client, openai_client=openai_client)\n",
    "    dom.setup()\n",
    "    ast_json_file = file.parent / (str(file.stem) + \"_ast.json\")\n",
    "    semantics_json_file = file.parent / (str(file.stem) + \"_semantics.json\")\n",
    "    ast_json_file.write_text(dom.ast_json, encoding=\"utf-8\")\n",
    "    await dom.textualize()\n",
    "    semantics_json_file.write_text(dom.ast_json, encoding=\"utf-8\")\n",
    "    print(f\"Semantics analysis completed for {file}. Results saved to {semantics_json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def supervisor(root_folder: Path) -> int:\n",
    "    \"\"\"\n",
    "    Supervises the processing of Markdown files in a root folder.\n",
    "    \"\"\"\n",
    "\n",
    "    tasks = []\n",
    "    for file in root_folder.rglob(\"*.md\"):\n",
    "        print(f\"Processing file started: {file}\")\n",
    "        tasks.append(process_one(file))\n",
    "    \n",
    "    res = await asyncio.gather(*tasks)\n",
    "\n",
    "    return len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_many(root_folder: Path) -> None:\n",
    "    \"\"\"\n",
    "    Processes all Markdown files in a root folder recursively and generates their semantics analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    return asyncio.run(supervisor(root_folder))\n",
    "\n",
    "process_many(Path(\"../res/test_batch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e4302",
   "metadata": {},
   "source": [
    "## Section Class: Recursive Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd46fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73069f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify the dual-client system works\n",
    "if False:\n",
    "    # Test that our unified functions work with both client types\n",
    "    test_content = \"This is a test string for summarization.\"\n",
    "    \n",
    "    # Test with OpenAI client (should use text summarization)\n",
    "    openai_summary = await get_summary_response_async(openai_client, test_content)\n",
    "    print(f\"OpenAI summary: {openai_summary}\")\n",
    "    \n",
    "    # Test with Ollama client (should use Ollama)  \n",
    "    ollama_summary = await get_summary_response_async(ollama_client, test_content)\n",
    "    print(f\"Ollama summary: {ollama_summary}\")\n",
    "\n",
    "# Test DOM with dual clients\n",
    "if False:\n",
    "    test_file = Path(\"../res/siasun_md_sample/test.md\")\n",
    "    if test_file.exists():\n",
    "        dom = DOM(test_file, ollama_client=ollama_client, openai_client=openai_client)\n",
    "        dom.setup()\n",
    "        print(f\"DOM created successfully with dual clients!\")\n",
    "        print(f\"Ollama client type: {type(dom.ollama_client)}\")  \n",
    "        print(f\"OpenAI client type: {type(dom.openai_client)}\")\n",
    "    else:\n",
    "        print(f\"Test file {test_file} not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
