{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2919343d",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: local_rag_ds_r1.html\n",
    "title: Gradio RAG tutorial with langchain and Google GenAI interface\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "68340256",
   "metadata": {},
   "source": "# |default_exp local_rag_ds_r1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15fe30a9-cfeb-4ec0-ae40-334072046464",
   "metadata": {},
   "source": [
    "Please reference [this blog post](https://nbdev.fast.ai/blog/posts/2022-11-07-spaces) on how to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7644f-42b3-4d8d-a1d0-f8c3a2358e27",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca22d1e-1bd0-49c0-9b89-c480ad1a29c4",
   "metadata": {},
   "source": [
    "## Make an app with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "id": "00cf4fad-a920-41dc-be42-3992c7fcefac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:21:33.984545Z",
     "start_time": "2025-04-18T06:21:33.203907Z"
    }
   },
   "source": [
    "# |export\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import time"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/.pyenv/versions/miniconda3-latest/envs/cell/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1cc7c77a267305b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:21:38.177971Z",
     "start_time": "2025-04-18T06:21:37.930744Z"
    }
   },
   "source": [
    "#| export\n",
    "import ollama\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "import getpass"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "9167da2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:21:41.307192Z",
     "start_time": "2025-04-18T06:21:41.304953Z"
    }
   },
   "source": [
    "# |export\n",
    "load_dotenv()\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:20171'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:20171'\n",
    "os.environ['NO_PROXY'] = 'localhost, 127.0.0.1'\n",
    "\n",
    "sys.getdefaultencoding()\n",
    "print(os.environ.get('HTTP_PROXY'))\n",
    "print(os.environ.get('HTTPS_PROXY'))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:20171\n",
      "http://127.0.0.1:20171\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "d292f13e1c1884d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:21:55.063429Z",
     "start_time": "2025-04-18T06:21:51.218671Z"
    }
   },
   "source": [
    "#| export\n",
    "# file_path = \"../res/employee_manual.pdf\"\n",
    "file_path = \"../res/foundatiuons_of_llm_zhu.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "print(len(documents))\n",
    "# print(documents[0].page_content[0:1000])\n",
    "print(documents[0].metadata)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "{'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': '../res/foundatiuons_of_llm_zhu.pdf', 'total_pages': 231, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9cccf421132720d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:21:59.643781Z",
     "start_time": "2025-04-18T06:21:59.631881Z"
    }
   },
   "source": [
    "#| export\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:23:21.650203Z",
     "start_time": "2025-04-18T06:22:06.564699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "# Initialize Ollama embeddings using DeepSeek-R1, default to 7b\n",
    "embedding_function = OllamaEmbeddings(model=\"deepseek-r1:14b\")\n",
    "# Parallelize embedding generation\n",
    "def generate_embeddings(chunk):\n",
    "    # return embedding_function.embed_query(chunk.page_content)\n",
    "    return embedding_function.embed_documents([chunk])[0]\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    embeddings = list(executor.map(generate_embeddings, chunks))\n"
   ],
   "id": "4b49e3c2b0026e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_476601/129587508.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_function = OllamaEmbeddings(model=\"deepseek-r1:14b\")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:23:28.635210Z",
     "start_time": "2025-04-18T06:23:27.266857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "# Initialize Chroma client and create/reset the collection\n",
    "client = Client(Settings())\n",
    "# client.delete_collection(name=\"foundations_of_llm\")\n",
    "collection = client.create_collection(name=\"foundations_of_llm\")\n",
    "# Add documents to the Chroma\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    collection.add(\n",
    "        documents=[chunk.page_content],\n",
    "        metadatas=[{'id': idx}],\n",
    "        embeddings=[embeddings[idx]],\n",
    "        ids=[str(idx)],\n",
    "    )"
   ],
   "id": "aded6be79845c422",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:23:35.358586Z",
     "start_time": "2025-04-18T06:23:35.355214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "# Initialize retriever using Ollama embeddings for queries\n",
    "retriever = Chroma(collection_name=\"foundations_of_llm\",client=client,embedding_function=embedding_function).as_retriever()"
   ],
   "id": "a353850aaadd18bc",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:23:52.084491Z",
     "start_time": "2025-04-18T06:23:52.082198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "def retrieve_context(question):\n",
    "    # Retrieve relevant documents\n",
    "    results = retriever.invoke(question)\n",
    "    # Combine the retrieved content\n",
    "    context = '\\n\\n'.join([doc.page_content for doc in results])\n",
    "    return context\n"
   ],
   "id": "eef50ffe8aa0f66f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c0a7d8e09f399d98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:10.160570Z",
     "start_time": "2025-04-18T06:24:10.157888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "# Query DeepSeek-R1 model for contextual answers\n",
    "def query_deepseek(question, context):\n",
    "    # Format the input prompt\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    # Query the model using Ollama\n",
    "    # response = embedding_function.chat(\n",
    "    #     model=\"deepseek-r1:7b\",\n",
    "    #     messages=[{'role': 'user', 'content': formatted_prompt}],\n",
    "    # )\n",
    "    response = ollama.chat(\n",
    "        model=\"deepseek-r1:14b\",\n",
    "        messages=[{'role': 'user', 'content': formatted_prompt}]\n",
    "    )\n",
    "    # Clean and return the response\n",
    "    response_content = response['message']['content']\n",
    "    final_answer = re.sub(r'<think>.*?</think>', '', response_content, flags=re.DOTALL).strip()\n",
    "    return final_answer"
   ],
   "id": "97c5c60274e84a52",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:13.950400Z",
     "start_time": "2025-04-18T06:24:13.948198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "def ask_question(question):\n",
    "    # Retrieve context and generate an answer using RAG\n",
    "    context = retrieve_context(question)\n",
    "    # Query DeepSeek-R1 model\n",
    "    answer = query_deepseek(question, context)\n",
    "    return answer\n",
    "    # return \"This is a test answer for the question: \" + question\n",
    "    # return \"Context: \" + context"
   ],
   "id": "bba9193ee4d3ba4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:36.450563Z",
     "start_time": "2025-04-18T06:24:23.089366Z"
    }
   },
   "cell_type": "code",
   "source": "ask_question(\"What is the main idea of the document?\")",
   "id": "71413c4807ccef8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### 1. Main Idea of the Document:  \\nThe main idea of the document is about **efficient variants of BERT models** and methods to improve their performance while reducing computational costs. It discusses techniques like knowledge distillation, parameter-efficient fine-tuning (e.g., prefix tuning), and learning soft prompts for adapting large language models (LLMs) to various tasks.\\n\\n---\\n\\n### 2. Extracted Financial Figures:  \\n- **Revenue**: $10 million  \\n- **Profit Margin**: 15%  \\n\\n---\\n\\n### 3. Spam Classification:  \\nThe email is classified as **spam** because it contains a common phishing/scheme pattern, such as announcing an unexpected prize (a gift card) and prompting the recipient to click on a link to claim it.\\n\\n---\\n\\n### 4. Technical Issue Solution:  \\nTo address the issue of a slow computer that often freezes, follow these steps:\\n\\n1. **Close Unnecessary Background Processes**:  \\n   - Restart your computer and avoid running too many applications at once. Use Task Manager (Windows) or Activity Monitor (Mac) to identify and close unnecessary processes.\\n\\n2. **Restart in Safe Mode**:  \\n   - Boot your computer in Safe Mode to see if the issue persists. If it runs smoothly in Safe Mode, disable startup programs that may be causing lag.\\n\\n3. **Clean Up Your Disk**:  \\n   - Use disk cleanup tools (e.g., Windows' Disk Cleanup or Mac's OnyX) to remove unnecessary files and optimize storage.\\n\\n4. **Check for Malware**:  \\n   - Install reputable antivirus software and run a full system scan to identify and remove any malicious programs.\\n\\n5. **Update Drivers and Software**:  \\n   - Ensure your operating system, drivers, and applications are up to date. Outdated drivers can cause system instability.\\n\\n6. **Increase RAM or Free Up Disk Space**:  \\n   - If your computer has insufficient RAM, consider upgrading it. Also, ensure there is at least 10-20 GB of free disk space on your primary drive.\\n\\n7. **Check for Hardware Issues**:  \\n   - Overheating can cause a computer to slow down or freeze. Ensure fans are working properly and vents are not blocked. Consider cleaning dust buildup inside the device (if applicable).\\n\\n8. **Perform a Clean Install**:  \\n   - If none of the above works, back up your data and perform a clean installation of your operating system.\\n\\n---\\n\\n### 5. Research Paper Summary:  \\nThe document discusses efficient approaches to BERT model optimization, focusing on methods like knowledge distillation and parameter-efficient fine-tuning (e.g., prefix tuning). It highlights how these techniques enable smaller or more resource-efficient models while maintaining performance. For example, knowledge distillation involves training student models using the output of larger teacher models, and prefix fine-tuning appends trainable vectors to input layers to guide model behavior without updating all parameters. These methods are particularly useful for adapting large language models to specific tasks efficiently.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:39:25.769514Z",
     "start_time": "2025-04-18T06:39:25.654785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Set up Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ask_question,\n",
    "    inputs=\"text\", #gr.Textbox(label=\"Ask a question about the document\"),\n",
    "    outputs=\"text\", #gr.Textbox(label=\"Answer\"),\n",
    "    title=\"RAG with DeepSeek-R1: Foundations of LLM\",\n",
    "    description=\"RAG with DeepSeek-R1: Foundations of LLM\",\n",
    ")\n",
    "interface.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)"
   ],
   "id": "ded5c401a4447da3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:49:25.586205Z",
     "start_time": "2025-04-18T06:49:25.401914Z"
    }
   },
   "cell_type": "code",
   "source": "interface.close()",
   "id": "f6a9225e2c4f5fc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'How does the distilled model perform in the evaluation comparing to other language models?', 'context': [Document(id='af74dba9-f877-492a-b88a-0c5d89dea874', metadata={'author': '', 'creationdate': '2025-01-23T01:45:31+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '../res/DeepSeek_R1.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': '/False'}, page_content='R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\\nreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-\\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\\nexceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-\\ntion. Additionally, we found that applying RL to these distilled models yields significant further\\ngains. We believe this warrants further exploration and therefore present only the results of the\\nsimple SFT-distilled models here.\\n4. Discussion\\n4.1. Distillation v.s. Reinforcement Learning\\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive\\nresults. However, there is still one question left: can the model achieve comparable performance\\nthrough the large-scale RL training discussed in the paper without distillation?'), Document(id='3bd2ff72-dbb3-4a73-a5fe-3778d254ccdf', metadata={'author': '', 'creationdate': '2025-01-23T01:45:31+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '../res/DeepSeek_R1.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': '/False'}, page_content='R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\\nreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-\\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\\nexceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-\\ntion. Additionally, we found that applying RL to these distilled models yields significant further\\ngains. We believe this warrants further exploration and therefore present only the results of the\\nsimple SFT-distilled models here.\\n4. Discussion\\n4.1. Distillation v.s. Reinforcement Learning\\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive\\nresults. However, there is still one question left: can the model achieve comparable performance\\nthrough the large-scale RL training discussed in the paper without distillation?'), Document(id='90d443a3-7203-4e8a-ad3b-8fb5b8751039', metadata={'author': '', 'creationdate': '2025-01-23T01:45:31+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'page': 14, 'page_label': '15', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '../res/DeepSeek_R1.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': '/False'}, page_content='Model\\nAIME 2024 MATH-500 GPQA Diamond LiveCodeBench\\npass@1 cons@64 pass@1 pass@1 pass@1\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9\\nDeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2\\nTable 6 |Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks.\\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller\\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\\nthis paper require enormous computational power and may not even achieve the performance\\nof distillation. Second, while distillation strategies are both economical and effective, advancing'), Document(id='8d87e3f4-55d6-4c8d-8d75-6088e2462a9f', metadata={'author': '', 'creationdate': '2025-01-23T01:45:31+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'page': 14, 'page_label': '15', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '../res/DeepSeek_R1.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': '/False'}, page_content='Model\\nAIME 2024 MATH-500 GPQA Diamond LiveCodeBench\\npass@1 cons@64 pass@1 pass@1 pass@1\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9\\nDeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2\\nTable 6 |Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks.\\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller\\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\\nthis paper require enormous computational power and may not even achieve the performance\\nof distillation. Second, while distillation strategies are both economical and effective, advancing')], 'answer': 'The distilled model, specifically DeepSeek-R1-Distill-Qwen-32B, significantly outperforms the benchmarks compared to other language models like QwQ-32B-Preview on reasoning-related benchmarks. It achieves higher pass rates on benchmarks such as AIME 2024 and MATH-500. Overall, distillation has proven to be a more effective and economical strategy compared to relying solely on large-scale reinforcement learning.'}\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "#| export\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"How does the distilled model perform in the evaluation comparing to other language models?\"})\n",
    "# print(results['answer'])"
   ],
   "id": "7fedc9815862b597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distilled model, specifically DeepSeek-R1-Distill-Qwen-32B, significantly outperforms the benchmarks compared to other language models like QwQ-32B-Preview on reasoning-related benchmarks. It achieves higher pass rates on benchmarks such as AIME 2024 and MATH-500. Overall, distillation has proven to be a more effective and economical strategy compared to relying solely on large-scale reinforcement learning.\n"
     ]
    }
   ],
   "execution_count": null,
   "source": "print(results['answer'])",
   "id": "67b9af637442d561"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a24da5a1a14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "import validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4d67c64d6c76a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WebBaseLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[74]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m#| export\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# Load and chunk contents of the blog\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m loader = \u001B[43mWebBaseLoader\u001B[49m(\n\u001B[32m      4\u001B[39m     web_paths=(\u001B[33m\"\u001B[39m\u001B[33mhttps://lilianweng.github.io/posts/2023-06-23-agent/\u001B[39m\u001B[33m\"\u001B[39m,),\n\u001B[32m      5\u001B[39m     bs_kwargs=\u001B[38;5;28mdict\u001B[39m(\n\u001B[32m      6\u001B[39m         parse_only=bs4.SoupStrainer(\n\u001B[32m      7\u001B[39m             class_=(\u001B[33m\"\u001B[39m\u001B[33mpost-content\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mpost-title\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mpost-header\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      8\u001B[39m         )\n\u001B[32m      9\u001B[39m     ),\n\u001B[32m     10\u001B[39m )\n\u001B[32m     11\u001B[39m docs = loader.load()\n\u001B[32m     13\u001B[39m text_splitter = RecursiveCharacterTextSplitter(chunk_size=\u001B[32m1000\u001B[39m, chunk_overlap=\u001B[32m200\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'WebBaseLoader' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vectorstore.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096eb913c1404ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2b3cf01b2fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, manageable steps or subgoals. This can be achieved through techniques like Chain of Thought (CoT), which encourages the model to think step-by-step, or by creating a Tree of Thoughts that explores multiple reasoning possibilities for each step. It can be initiated through simple prompts, specific instructions, or human inputs.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083cd73fabb9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def answer(message, history, system_prompt, tokens):\n",
    "    files = []\n",
    "    file_names = []\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\" and isinstance(msg[\"content\"], tuple):\n",
    "            files.append(msg[\"content\"][0])\n",
    "            file_names.append(msg[\"content\"][0].split(\"/\")[-1])\n",
    "    for file in message[\"files\"]:\n",
    "        files.append(file)\n",
    "        file_names.append(file.split(\"/\")[-1])\n",
    "\n",
    "    #if message[\"text\"]:\n",
    "    #    content = message[\"text\"]\n",
    "    #else:\n",
    "    #    content = system_prompt\n",
    "    # content = message\n",
    "    # question = system_prompt\n",
    "    # response = f\"Content: {content}\\nQuestion: {question}\\n\"\n",
    "    # len = min(len(response),int(response_len))\n",
    "\n",
    "    user_input = f\"Question: {system_prompt}\\n Website: {message['text']}\\n File:\\n{'\\n'.join(file_names)}\"\n",
    "\n",
    "    if validators.url(message['text']):\n",
    "        loader = WebBaseLoader(\n",
    "            # web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "            web_paths=(message['text'],),\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        all_splits = text_splitter.split_documents(docs)\n",
    "        # Index chunks\n",
    "        _ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "        # # # Compile application and test\n",
    "        # graph_builder_i = StateGraph(State).add_sequence([retrieve, generate])\n",
    "        # graph_builder_i.add_edge(START, \"retrieve\")\n",
    "        # graph_i = graph_builder_i.compile()\n",
    "        reply = graph.invoke({\"question\": system_prompt})\n",
    "        response_i = reply[\"answer\"]\n",
    "    elif files:\n",
    "        f = files[-1]\n",
    "        f_name = file_names[-1]\n",
    "        loader = PyPDFLoader(f)\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        vectorstore = Chroma.from_documents(documents=splits,embedding=OpenAIEmbeddings())\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "            \"Use the following pieces of retrieved context to answer \"\n",
    "            \"the question. If you don't know the answer, say that you \"\n",
    "            \"don't know. Use three sentences maximum and keep the \"\n",
    "            \"answer concise.\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "        )\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "        rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "        results = rag_chain.invoke({\"input\": system_prompt})\n",
    "\n",
    "        response_i = results[\"answer\"]\n",
    "\n",
    "    # response_i = user_input\n",
    "    for i in range(min(len(response_i), int(tokens))):\n",
    "        time.sleep(0.05)\n",
    "        yield response_i[: i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20e2a1-b622-4970-9069-0202ce10a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "demo = gr.ChatInterface(\n",
    "    answer,\n",
    "    type=\"messages\",\n",
    "    title=\"智能问答RAG\",\n",
    "    description=\"输入一个网址，查询或询问其中的内容。\",\n",
    "    textbox=gr.MultimodalTextbox(value=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                                 file_count=\"multiple\",\n",
    "                                 file_types=[\"image\", \".pdf\", \".txt\"],\n",
    "                                 sources=[\"upload\", \"microphone\"]),\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(\"What is Task Decomposition?\", label=\"你的问题在此输入！\"),\n",
    "        gr.Slider(10,400,value=300,label=\"回答长度\")\n",
    "    ],\n",
    "    multimodal=True,\n",
    ")\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7be72-9389-42cf-91b1-78e8f4bbd083",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'demo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[72]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# this is only necessary in a notebook\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mdemo\u001B[49m.close()\n",
      "\u001B[31mNameError\u001B[39m: name 'demo' is not defined"
     ]
    }
   ],
   "source": [
    "# this is only necessary in a notebook\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88424f53-cd78-41fe-9e06-8a6209001064",
   "metadata": {},
   "source": [
    "## Create a `requirements.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a30aa-9090-460e-acf9-4eb359161125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../requirements.txt\n",
    "fastcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b2cd7-3123-45bf-945f-882b8a964cf5",
   "metadata": {},
   "source": [
    "## Convert this notebook into a Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706d92c-5785-4f09-9773-b9a944c493a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev.export import nb_export\n",
    "# nb_export('01_gradio.ipynb', lib_path='.', name='gradio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182403f-d1d6-48c0-8e66-46aefb23a9ab",
   "metadata": {},
   "source": [
    "<div>\n",
    "<link rel=\"stylesheet\" href=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.css\">\n",
    "<div id=\"target\"></div>\n",
    "<script src=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.js\"></script>\n",
    "<script>\n",
    "launchGradioFromSpaces(\"abidlabs/question-answering\", \"#target\")\n",
    "</script>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
