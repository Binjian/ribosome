{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2919343d",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: langchain_rag_groq.html\n",
    "title: Gradio RAG tutorial with langchain and groq\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68340256",
   "metadata": {},
   "outputs": [],
   "source": "# |default_exp langchain_rag_groq"
  },
  {
   "cell_type": "markdown",
   "id": "15fe30a9-cfeb-4ec0-ae40-334072046464",
   "metadata": {},
   "source": [
    "Please reference [this blog post](https://nbdev.fast.ai/blog/posts/2022-11-07-spaces) on how to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7644f-42b3-4d8d-a1d0-f8c3a2358e27",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca22d1e-1bd0-49c0-9b89-c480ad1a29c4",
   "metadata": {},
   "source": [
    "## Make an app with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf4fad-a920-41dc-be42-3992c7fcefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2ba8d5179e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "load_dotenv()\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:20171'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:20171'\n",
    "# os.environ['NO_PROXY'] = 'localhost, 127.0.0.1'\n",
    "print(os.environ.get('HTTP_PROXY'))\n",
    "print(os.environ.get('HTTPS_PROXY'))\n",
    "print(os.environ.get('GROQ_API_KEY'))\n",
    "print(os.environ.get('OPIK_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebc1b160fdc1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from llama_index.core import Settings, SimpleDirectoryReader\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7c77a267305b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
    "llm = Groq(model= model_name, api_key= os.environ.get('GROQ_API_KEY'))\n",
    "Settings.llm = llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5be5433be7cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.core import Settings\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf408bb7ec80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "# embedding_model = 'nomic-ai/nomic-embed-text-v2-moe'\n",
    "embedding_model = 'nomic-ai/nomic-embed-text-v1'\n",
    "embed_model = FastEmbedEmbedding(model_name=embedding_model)\n",
    "Settings.embed_model = embed_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fedc9815862b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opik\n",
    "opik.configure(use_local=False)\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from opik.integrations.llama_index import LlamaIndexCallbackHandler\n",
    "\n",
    "callback_manager = CallbackManager([LlamaIndexCallbackHandler()])\n",
    "Settings.callback_manager = callback_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0c3a369eabf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event, Context, Workflow, StartEvent, StopEvent, step\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8b0c4d70a4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Add model mapping\n",
    "GROQ_MODELS = {\n",
    "    \"Llama 4\":\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    \"DeepSeek-R1\":\"deepseek-r1-distill-llama-70b\",\n",
    "}\n",
    "\n",
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "    \"\"\"RAG workflow\"\"\"\n",
    "\n",
    "    def __init__(self, llm_choice=\"Llama 4\", embedding_model=\"BAAI/bge-large-en-v1.5\"):\n",
    "        super().__init__()\n",
    "        model_name = GROQ_MODELS.get(llm_choice)\n",
    "        if not model_name:\n",
    "            raise ValueError(\"Invalid LLM choice. Please select from:{', '.join(GROQ_MODELS.keys())}\")\n",
    "\n",
    "        #Initialize LLM and embedding model\n",
    "        self.llm = Groq(model= model_name, api_key= os.environ.get('GROQ_API_KEY'))\n",
    "        self.embed_model = FastEmbedEmbedding(model_name=embedding_model)\n",
    "\n",
    "        self.index = None\n",
    "\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev:StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Entry point to ingest documents from a directory\"\"\"\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "\n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        self.index = VectorStoreIndex.from_documents(documents=documents)\n",
    "        return StopEvent(result=self.index)\n",
    "\n",
    "    @step\n",
    "    async def retrieve(self, ctx: Context, ev: StartEvent) -> RetrieverEvent:\n",
    "        \"\"\"Entry point fro RAG retrieval\"\"\"\n",
    "        query = ev.get(\"query\")\n",
    "        index = ev.get(\"index\") or self.index\n",
    "\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        if index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        retriever = index.as_retriever(similarity_top_k=2)\n",
    "        nodes = await retriever.aretrieve(query)\n",
    "        await ctx.set(\"query\", query)\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "\n",
    "    @step\n",
    "    async def synthesize(self, ctx: Context, ev: RetrieverEvent) -> StopEvent:\n",
    "        \"\"\"Generate response using retrieved nodes.\"\"\"\n",
    "        summarizer = CompactAndRefine(streaming=True, verbose=True)\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(response=response)\n",
    "\n",
    "    async def query(self, query_text:str):\n",
    "        \"\"\"Helper method to perform a complete RAG query\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index is empty. Call ingest_document first!\")\n",
    "\n",
    "        result = await self.run(query=query_text, index=self.index)\n",
    "        if not result:\n",
    "            raise ValueError(\"No result returned from query!\")\n",
    "        return result\n",
    "\n",
    "    async def ingest_documents(self, directory: str):\n",
    "        \"\"\"Helper method to ingest documents\"\"\"\n",
    "        result = await self.run(dirname=directory)\n",
    "        self.index = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8890f7a637c7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "draw_all_possible_flows(RAGWorkflow, filename=\"crag_workflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caaebac74edca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "workflow = RAGWorkflow(llm_choice=\"Llama 4\") #, embedding_model=\"nomic-ai/nomic-embed-text-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fb6748818cbfb",
   "metadata": {},
   "outputs": [],
   "source": "await workflow.ingest_documents(\"../data\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29043d9815ed3550",
   "metadata": {},
   "outputs": [],
   "source": "result = await workflow.run(query=\"How was DeepSeek R1 trained?\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec55b25ecda748",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await workflow.run(query=\"How was DeepSeek R1 trained?\")\n",
    "display(Markdown(str(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ceff98d1fc9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "async def main():\n",
    "    # Initialize the workflow\n",
    "    print(\"Set up workflow ...\")\n",
    "    workflow = RAGWorkflow(llm_choice=\"Llama 4\")\n",
    "    print(\"start ingest ...\")\n",
    "    await workflow.ingest_documents(\"../data\")\n",
    "\n",
    "    # Perform a query\n",
    "    print(\"start query ...\")\n",
    "    result = await workflow.query(\"How was DeepSeek R1 trained?\")\n",
    "\n",
    "    # Print the response\n",
    "    async for chunk in result.async_response_gen():\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88361abedac7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083cd73fabb9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def answer(message, history, system_prompt, tokens):\n",
    "    files = []\n",
    "    file_names = []\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\" and isinstance(msg[\"content\"], tuple):\n",
    "            files.append(msg[\"content\"][0])\n",
    "            file_names.append(msg[\"content\"][0].split(\"/\")[-1])\n",
    "    for file in message[\"files\"]:\n",
    "        files.append(file)\n",
    "        file_names.append(file.split(\"/\")[-1])\n",
    "\n",
    "    #if message[\"text\"]:\n",
    "    #    content = message[\"text\"]\n",
    "    #else:\n",
    "    #    content = system_prompt\n",
    "    # content = message\n",
    "    # question = system_prompt\n",
    "    # response = f\"Content: {content}\\nQuestion: {question}\\n\"\n",
    "    # len = min(len(response),int(response_len))\n",
    "\n",
    "    user_input = f\"Question: {system_prompt}\\n Website: {message['text']}\\n File:\\n{'\\n'.join(file_names)}\"\n",
    "\n",
    "    if validators.url(message['text']):\n",
    "        loader = WebBaseLoader(\n",
    "            # web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "            web_paths=(message['text'],),\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        all_splits = text_splitter.split_documents(docs)\n",
    "        # Index chunks\n",
    "        _ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "        # # # Compile application and test\n",
    "        # graph_builder_i = StateGraph(State).add_sequence([retrieve, generate])\n",
    "        # graph_builder_i.add_edge(START, \"retrieve\")\n",
    "        # graph_i = graph_builder_i.compile()\n",
    "        reply = graph.invoke({\"question\": system_prompt})\n",
    "        response_i = reply[\"answer\"]\n",
    "    elif files:\n",
    "        f = files[-1]\n",
    "        f_name = file_names[-1]多久维护一次产品?\n",
    "        response_i = f\"File: {f_name}\\n\"\n",
    "\n",
    "    # response_i = user_input\n",
    "    for i in range(min(len(response_i), int(tokens))):\n",
    "        time.sleep(0.05)\n",
    "        yield response_i[: i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20e2a1-b622-4970-9069-0202ce10a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "demo = gr.ChatInterface(\n",
    "    answer,\n",
    "    type=\"messages\",\n",
    "    title=\"智能问答RAG\",\n",
    "    description=\"输入一个网址，查询或询问其中的内容。\",\n",
    "    textbox=gr.MultimodalTextbox(value=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                                 file_count=\"multiple\",\n",
    "                                 file_types=[\"image\", \".pdf\", \".txt\"],\n",
    "                                 sources=[\"upload\", \"microphone\"]),\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(\"What is Task Decomposition?\", label=\"你的问题在此输入！\"),\n",
    "        gr.Slider(10,400,value=300,label=\"回答长度\")\n",
    "    ],\n",
    "    multimodal=True,\n",
    ")\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7be72-9389-42cf-91b1-78e8f4bbd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only necessary in a notebook\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88424f53-cd78-41fe-9e06-8a6209001064",
   "metadata": {},
   "source": [
    "## Create a `requirements.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a30aa-9090-460e-acf9-4eb359161125",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../requirements.txt\n",
    "fastcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b2cd7-3123-45bf-945f-882b8a964cf5",
   "metadata": {},
   "source": [
    "## Convert this notebook into a Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706d92c-5785-4f09-9773-b9a944c493a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev.export import nb_export\n",
    "# nb_export('01_gradio.ipynb', lib_path='.', name='gradio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182403f-d1d6-48c0-8e66-46aefb23a9ab",
   "metadata": {},
   "source": [
    "<div>\n",
    "<link rel=\"stylesheet\" href=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.css\">\n",
    "<div id=\"target\"></div>\n",
    "<script src=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.js\"></script>\n",
    "<script>\n",
    "launchGradioFromSpaces(\"abidlabs/question-answering\", \"#target\")\n",
    "</script>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
