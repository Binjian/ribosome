{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2919343d",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: langchain_rag_groq.html\n",
    "title: Gradio RAG tutorial with langchain and groq\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "68340256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T05:57:23.306643Z",
     "start_time": "2025-04-07T05:57:23.303109Z"
    }
   },
   "source": "# |default_exp langchain_rag_groq",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "15fe30a9-cfeb-4ec0-ae40-334072046464",
   "metadata": {},
   "source": [
    "Please reference [this blog post](https://nbdev.fast.ai/blog/posts/2022-11-07-spaces) on how to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7644f-42b3-4d8d-a1d0-f8c3a2358e27",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca22d1e-1bd0-49c0-9b89-c480ad1a29c4",
   "metadata": {},
   "source": [
    "## Make an app with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "id": "00cf4fad-a920-41dc-be42-3992c7fcefac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:13:57.217027Z",
     "start_time": "2025-04-07T06:13:55.529460Z"
    }
   },
   "source": [
    "# |export\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import getpass"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:37:36.323813Z",
     "start_time": "2025-04-07T06:37:36.319266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# |export\n",
    "load_dotenv()\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:20171'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:20171'\n",
    "# os.environ['NO_PROXY'] = 'localhost, 127.0.0.1'\n",
    "print(os.environ.get('HTTP_PROXY'))\n",
    "print(os.environ.get('HTTPS_PROXY'))\n",
    "print(os.environ.get('GROQ_API_KEY'))\n",
    "print(os.environ.get('OPIK_API_KEY'))\n"
   ],
   "id": "81d2ba8d5179e227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:20171\n",
      "http://127.0.0.1:20171\n",
      "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\n",
      "ayPG5KCTBSO81Bab9QcMofWjF\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:14:05.788367Z",
     "start_time": "2025-04-07T06:14:03.340292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "from llama_index.core import Settings, SimpleDirectoryReader\n",
    "from llama_index.llms.groq import Groq"
   ],
   "id": "a0ebc1b160fdc1a5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:14:06.821937Z",
     "start_time": "2025-04-07T06:14:06.817094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model_name = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
    "llm = Groq(model= model_name, api_key= os.environ.get('GROQ_API_KEY'))\n",
    "Settings.llm = llm\n"
   ],
   "id": "1cc7c77a267305b2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:31:09.090493Z",
     "start_time": "2025-04-07T06:31:09.088391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.core import Settings\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ],
   "id": "ff5be5433be7cccf",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:34:48.182383Z",
     "start_time": "2025-04-07T06:34:44.181600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "# embedding_model = 'nomic-ai/nomic-embed-text-v2-moe'\n",
    "embedding_model = 'nomic-ai/nomic-embed-text-v1'\n",
    "embed_model = FastEmbedEmbedding(model_name=embedding_model)\n",
    "Settings.embed_model = embed_model\n"
   ],
   "id": "4dcf408bb7ec80be",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "7fedc9815862b597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:40:36.447579Z",
     "start_time": "2025-04-07T06:40:23.060510Z"
    }
   },
   "source": [
    "import opik\n",
    "opik.configure(use_local=False)\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from opik.integrations.llama_index import LlamaIndexCallbackHandler\n",
    "\n",
    "callback_manager = CallbackManager([LlamaIndexCallbackHandler()])\n",
    "Settings.callback_manager = callback_manager"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Configuration saved to file: /home/x/.opik.config\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:14:31.790749Z",
     "start_time": "2025-04-07T07:14:31.787774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.workflow import Event, Context, Workflow, StartEvent, StopEvent, step\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "id": "f3b0c3a369eabf33",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:43:17.064622Z",
     "start_time": "2025-04-07T07:43:16.994029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "\n",
    "# Add model mapping\n",
    "GROQ_MODELS = {\n",
    "    \"Llama 4\":\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    \"DeepSeek-R1\":\"deepseek-r1-distill-llama-70b\",\n",
    "}\n",
    "\n",
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "    \"\"\"RAG workflow\"\"\"\n",
    "\n",
    "    def __init__(self, llm_choice=\"Llama 4\", embedding_model=\"BAAI/bge-large-en-v1.5\"):\n",
    "        super().__init__()\n",
    "        model_name = GROQ_MODELS.get(llm_choice)\n",
    "        if not model_name:\n",
    "            raise ValueError(\"Invalid LLM choice. Please select from:{', '.join(GROQ_MODELS.keys())}\")\n",
    "\n",
    "        #Initialize LLM and embedding model\n",
    "        self.llm = Groq(model= model_name, api_key= os.environ.get('GROQ_API_KEY'))\n",
    "        self.embed_model = FastEmbedEmbedding(model_name=embedding_model)\n",
    "\n",
    "        self.index = None\n",
    "\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev:StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Entry point to ingest documents from a directory\"\"\"\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "\n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        self.index = VectorStoreIndex.from_documents(documents=documents)\n",
    "        return StopEvent(result=self.index)\n",
    "\n",
    "    @step\n",
    "    async def retrieve(self, ctx: Context, ev: StartEvent) -> RetrieverEvent:\n",
    "        \"\"\"Entry point fro RAG retrieval\"\"\"\n",
    "        query = ev.get(\"query\")\n",
    "        index = ev.get(\"index\") or self.index\n",
    "\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        if index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        retriever = index.as_retriever(similarity_top_k=2)\n",
    "        nodes = await retriever.aretrieve(query)\n",
    "        await ctx.set(\"query\", query)\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "\n",
    "    @step\n",
    "    async def synthesize(self, ctx: Context, ev: RetrieverEvent) -> StopEvent:\n",
    "        \"\"\"Generate response using retrieved nodes.\"\"\"\n",
    "        summarizer = CompactAndRefine(streaming=True, verbose=True)\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(response=response)\n",
    "\n",
    "    async def query(self, query_text:str):\n",
    "        \"\"\"Helper method to perform a complete RAG query\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index is empty. Call ingest_document first!\")\n",
    "\n",
    "        result = await self.run(query=query_text, index=self.index)\n",
    "        if not result:\n",
    "            raise ValueError(\"No result returned from query!\")\n",
    "        return result\n",
    "\n",
    "    async def ingest_documents(self, directory: str):\n",
    "        \"\"\"Helper method to ingest documents\"\"\"\n",
    "        result = await self.run(dirname=directory)\n",
    "        self.index = result\n",
    "        return result"
   ],
   "id": "18c8b0c4d70a4f36",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:52:22.793714Z",
     "start_time": "2025-04-07T07:51:57.031670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "draw_all_possible_flows(RAGWorkflow, filename=\"crag_workflow.html\")"
   ],
   "id": "f8890f7a637c7889",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crag_workflow.html\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:56:00.550983Z",
     "start_time": "2025-04-07T07:55:51.075719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Markdown\n",
    "workflow = RAGWorkflow(llm_choice=\"Llama 4\") #, embedding_model=\"nomic-ai/nomic-embed-text-v1\")"
   ],
   "id": "2caaebac74edca49",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:57:41.890464Z",
     "start_time": "2025-04-07T07:57:32.785939Z"
    }
   },
   "cell_type": "code",
   "source": "await workflow.ingest_documents(\"../data\")",
   "id": "ce7fb6748818cbfb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7f80627c4230>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:00:13.288891Z",
     "start_time": "2025-04-07T08:00:13.245090Z"
    }
   },
   "cell_type": "code",
   "source": "result = await workflow.run(query=\"How was DeepSeek R1 trained?\")",
   "id": "29043d9815ed3550",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result = await workflow.run(query=\"How was DeepSeek R1 trained?\")\n",
    "display(Markdown(str(result)))"
   ],
   "id": "1eec55b25ecda748",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:47:01.336821Z",
     "start_time": "2025-04-07T07:47:01.334149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#|export\n",
    "async def main():\n",
    "    # Initialize the workflow\n",
    "    print(\"Set up workflow ...\")\n",
    "    workflow = RAGWorkflow(llm_choice=\"Llama 4\")\n",
    "    print(\"start ingest ...\")\n",
    "    await workflow.ingest_documents(\"../data\")\n",
    "\n",
    "    # Perform a query\n",
    "    print(\"start query ...\")\n",
    "    result = await workflow.query(\"How was DeepSeek R1 trained?\")\n",
    "\n",
    "    # Print the response\n",
    "    async for chunk in result.async_response_gen():\n",
    "        print(chunk, end=\"\", flush=True)"
   ],
   "id": "f6ceff98d1fc9b34",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:48:00.807888Z",
     "start_time": "2025-04-07T07:47:05.944065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(main())"
   ],
   "id": "fd88361abedac7b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up workflow ...\n",
      "start ingest ...\n",
      "start query ...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'async_response_gen'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[39]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m      2\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01masyncio\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     \u001B[43masyncio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/miniconda3-3.12-25.1.1-2/envs/pinecone/lib/python3.12/site-packages/nest_asyncio.py:30\u001B[39m, in \u001B[36m_patch_asyncio.<locals>.run\u001B[39m\u001B[34m(main, debug)\u001B[39m\n\u001B[32m     28\u001B[39m task = asyncio.ensure_future(main)\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task.done():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/miniconda3-3.12-25.1.1-2/envs/pinecone/lib/python3.12/site-packages/nest_asyncio.py:98\u001B[39m, in \u001B[36m_patch_loop.<locals>.run_until_complete\u001B[39m\u001B[34m(self, future)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f.done():\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m     97\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mEvent loop stopped before Future completed.\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/miniconda3-3.12-25.1.1-2/envs/pinecone/lib/python3.12/asyncio/futures.py:202\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    200\u001B[39m \u001B[38;5;28mself\u001B[39m.__log_traceback = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    201\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m202\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception.with_traceback(\u001B[38;5;28mself\u001B[39m._exception_tb)\n\u001B[32m    203\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/miniconda3-3.12-25.1.1-2/envs/pinecone/lib/python3.12/asyncio/tasks.py:314\u001B[39m, in \u001B[36mTask.__step_run_and_handle_result\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    311\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    312\u001B[39m         \u001B[38;5;66;03m# We use the `send` method directly, because coroutines\u001B[39;00m\n\u001B[32m    313\u001B[39m         \u001B[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m314\u001B[39m         result = \u001B[43mcoro\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    315\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    316\u001B[39m         result = coro.throw(exc)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[38]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     11\u001B[39m result = \u001B[38;5;28;01mawait\u001B[39;00m workflow.query(\u001B[33m\"\u001B[39m\u001B[33mHow was DeepSeek R1 trained?\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Print the response\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[43mresult\u001B[49m\u001B[43m.\u001B[49m\u001B[43masync_response_gen\u001B[49m():\n\u001B[32m     15\u001B[39m     \u001B[38;5;28mprint\u001B[39m(chunk, end=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m, flush=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'NoneType' object has no attribute 'async_response_gen'"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#| export\n",
    "def answer(message, history, system_prompt, tokens):\n",
    "    files = []\n",
    "    file_names = []\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\" and isinstance(msg[\"content\"], tuple):\n",
    "            files.append(msg[\"content\"][0])\n",
    "            file_names.append(msg[\"content\"][0].split(\"/\")[-1])\n",
    "    for file in message[\"files\"]:\n",
    "        files.append(file)\n",
    "        file_names.append(file.split(\"/\")[-1])\n",
    "\n",
    "    #if message[\"text\"]:\n",
    "    #    content = message[\"text\"]\n",
    "    #else:\n",
    "    #    content = system_prompt\n",
    "    # content = message\n",
    "    # question = system_prompt\n",
    "    # response = f\"Content: {content}\\nQuestion: {question}\\n\"\n",
    "    # len = min(len(response),int(response_len))\n",
    "\n",
    "    user_input = f\"Question: {system_prompt}\\n Website: {message['text']}\\n File:\\n{'\\n'.join(file_names)}\"\n",
    "\n",
    "    if validators.url(message['text']):\n",
    "        loader = WebBaseLoader(\n",
    "            # web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "            web_paths=(message['text'],),\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        all_splits = text_splitter.split_documents(docs)\n",
    "        # Index chunks\n",
    "        _ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "        # # # Compile application and test\n",
    "        # graph_builder_i = StateGraph(State).add_sequence([retrieve, generate])\n",
    "        # graph_builder_i.add_edge(START, \"retrieve\")\n",
    "        # graph_i = graph_builder_i.compile()\n",
    "        reply = graph.invoke({\"question\": system_prompt})\n",
    "        response_i = reply[\"answer\"]\n",
    "    elif files:\n",
    "        f = files[-1]\n",
    "        f_name = file_names[-1]多久维护一次产品?\n",
    "        response_i = f\"File: {f_name}\\n\"\n",
    "\n",
    "    # response_i = user_input\n",
    "    for i in range(min(len(response_i), int(tokens))):\n",
    "        time.sleep(0.05)\n",
    "        yield response_i[: i + 1]"
   ],
   "id": "8083cd73fabb9d13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20e2a1-b622-4970-9069-0202ce10a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "demo = gr.ChatInterface(\n",
    "    answer,\n",
    "    type=\"messages\",\n",
    "    title=\"智能问答RAG\",\n",
    "    description=\"输入一个网址，查询或询问其中的内容。\",\n",
    "    textbox=gr.MultimodalTextbox(value=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                                 file_count=\"multiple\",\n",
    "                                 file_types=[\"image\", \".pdf\", \".txt\"],\n",
    "                                 sources=[\"upload\", \"microphone\"]),\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(\"What is Task Decomposition?\", label=\"你的问题在此输入！\"),\n",
    "        gr.Slider(10,400,value=300,label=\"回答长度\")\n",
    "    ],\n",
    "    multimodal=True,\n",
    ")\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7be72-9389-42cf-91b1-78e8f4bbd083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7865\n"
     ]
    }
   ],
   "source": [
    "# this is only necessary in a notebook\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88424f53-cd78-41fe-9e06-8a6209001064",
   "metadata": {},
   "source": [
    "## Create a `requirements.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a30aa-9090-460e-acf9-4eb359161125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../requirements.txt\n",
    "fastcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b2cd7-3123-45bf-945f-882b8a964cf5",
   "metadata": {},
   "source": [
    "## Convert this notebook into a Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706d92c-5785-4f09-9773-b9a944c493a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev.export import nb_export\n",
    "# nb_export('01_gradio.ipynb', lib_path='.', name='gradio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182403f-d1d6-48c0-8e66-46aefb23a9ab",
   "metadata": {},
   "source": [
    "<div>\n",
    "<link rel=\"stylesheet\" href=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.css\">\n",
    "<div id=\"target\"></div>\n",
    "<script src=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.js\"></script>\n",
    "<script>\n",
    "launchGradioFromSpaces(\"abidlabs/question-answering\", \"#target\")\n",
    "</script>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
