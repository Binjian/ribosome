{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2919343d",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: langchain_rag.html\n",
    "title: Gradio RAG tutorial with langchain\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68340256",
   "metadata": {},
   "outputs": [],
   "source": "# |default_exp langchain_rag"
  },
  {
   "cell_type": "markdown",
   "id": "15fe30a9-cfeb-4ec0-ae40-334072046464",
   "metadata": {},
   "source": [
    "Please reference [this blog post](https://nbdev.fast.ai/blog/posts/2022-11-07-spaces) on how to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7644f-42b3-4d8d-a1d0-f8c3a2358e27",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca22d1e-1bd0-49c0-9b89-c480ad1a29c4",
   "metadata": {},
   "source": [
    "## Make an app with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf4fad-a920-41dc-be42-3992c7fcefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "from openai import api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7c77a267305b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.core.llms import ChatMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "load_dotenv()\n",
    "#os.environ['HTTP_PROXY'] = ''\n",
    "#os.environ['HTTPS_PROXY'] = ''\n",
    "#os.environ['NO_PROXY'] = 'localhost, 127.0.0.1'\n",
    "print(os.environ.get('OPENROUTER_API_KEY'))\n",
    "print(os.environ.get('OPENROUTER_API_URL'))\n",
    "print(os.environ.get('PINECONE_API_KEY'))\n",
    "print(os.environ.get('HTTP_PROXY'))\n",
    "print(os.environ.get('HTTPS_PROXY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8825809add8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec, Index\n",
    "import os\n",
    "import pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b97fef2bad768",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'quickstart'\n",
    "if dataset_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        dataset_name,\n",
    "        dimension=1536,\n",
    "        metric=\"euclidean\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "        )\n",
    "    ),\n",
    "pinecone_index = pc.Index(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267417287a3eb8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2dd1653a4c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embed data\n",
    "data = [\n",
    "    {\"id\": \"vec1\", \"text\": \"Apple is a popular fruit known for its sweetness and crisp texture.\"},\n",
    "    {\"id\": \"vec2\", \"text\": \"The tech company Apple is known for its innovative products like the iPhone.\"},\n",
    "    {\"id\": \"vec3\", \"text\": \"Many people enjoy eating apples as a healthy snack.\"},\n",
    "    {\"id\": \"vec4\", \"text\": \"Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.\"},\n",
    "    {\"id\": \"vec5\", \"text\": \"An apple a day keeps the doctor away, as the saying goes.\"},\n",
    "]\n",
    "\n",
    "embeddings = pc.inference.embed(\n",
    "    model=\"llama-text-embed-v2\",\n",
    "    inputs=[d['text'] for d in data],\n",
    "    parameters={\n",
    "        \"input_type\": \"passage\"\n",
    "    }\n",
    ")\n",
    "\n",
    "vectors = []\n",
    "for d, e in zip(data, embeddings):\n",
    "    vectors.append({\n",
    "        \"id\": d['id'],\n",
    "        \"values\": e['values'],\n",
    "        \"metadata\": {'text': d['text']}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd92364080e7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.upsert(\n",
    "    vectors=vectors,\n",
    "    namespace=\"ns1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b3dfa59e82b65",
   "metadata": {},
   "outputs": [],
   "source": "index.describe_index_stats()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77db9df3d720ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenRouter(\n",
    "    api_key=os.environ.get('OPENROUTER_API_KEY'),\n",
    "    max_tokens=256,\n",
    "    context_window=4096,\n",
    "    model=\"qwen/qwen2.5-vl-32b-instruct:free\"\n",
    "    # model=\"deepseek/deepseek-r1:free\"\n",
    "    # model=\"gryphe/mythomax-l2-13b:free\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff680b3471679e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from os import getenv\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=os.environ.get('OPENROUTER_API_URL'),\n",
    "    api_key=os.environ.get('OPENROUTER_API_KEY'),\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    # model=\"bytedance-research/ui-tars-72b:free\",\n",
    "    model=\"google/gemini-2.5-pro-exp-03-25:free\",\n",
    "    extra_headers={\n",
    "        \"HTTP-Referer\": \"binjian.github.io\",\n",
    "        \"X-Title\": \"My Test\",\n",
    "    },\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"给我讲个川普和普京的笑话吧.\"\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58185c30e957ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenRouter(\n",
    "    api_key=os.environ.get('OPENROUTER_API_KEY'),\n",
    "    max_tokens=256,\n",
    "    context_window=4096,\n",
    "    model=\"google/gemma-3-12b-it:free\"\n",
    "    # model=\"google/gemini-2.5-pro-exp-03-25:free\"\n",
    "    # model=\"deepseek/deepseek-r1:free\"\n",
    "    # model=\"gryphe/mythomax-l2-13b:free\"\n",
    ")\n",
    "message = ChatMessage(role=\"user\", content=\"Tell me a joke.\")\n",
    "resp = llm.chat([message])\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32993f368452573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ChatMessage(role=\"user\", content=\"请讲述一个250字的科幻小说故事\")\n",
    "resp = llm.stream_chat([message])\n",
    "for r in resp:\n",
    "    print(r.delta, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525dce9e6fa9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.complete(\"Tell me a joke in Feynman style.\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745999e02e4dca5",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454611c4239fe93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = ChatOpenAI(\n",
    "  openai_api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    "  openai_api_base=getenv(\"OPENROUTER_BASE_URL\"),\n",
    "  model_name=\"<model_name>\",\n",
    "  model_kwargs={\n",
    "    \"headers\": {\n",
    "      \"HTTP-Referer\": getenv(\"YOUR_SITE_URL\"),\n",
    "      \"X-Title\": getenv(\"YOUR_SITE_NAME\"),\n",
    "    }\n",
    "  },\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fedc9815862b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ChatMessage(role=\"user\", content=\"给我讲个少林寺笑话吧.\")\n",
    "resp = llm.chat(messages=[message])\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93763aa0099a0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afffa0b24b89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233045d32dc09b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a24da5a1a14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "import validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4d67c64d6c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096eb913c1404ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083cd73fabb9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def answer(message, history, system_prompt, tokens):\n",
    "    files = []\n",
    "    file_names = []\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\" and isinstance(msg[\"content\"], tuple):\n",
    "            files.append(msg[\"content\"][0])\n",
    "            file_names.append(msg[\"content\"][0].split(\"/\")[-1])\n",
    "    for file in message[\"files\"]:\n",
    "        files.append(file)\n",
    "        file_names.append(file.split(\"/\")[-1])\n",
    "\n",
    "    #if message[\"text\"]:\n",
    "    #    content = message[\"text\"]\n",
    "    #else:\n",
    "    #    content = system_prompt\n",
    "    # content = message\n",
    "    # question = system_prompt\n",
    "    # response = f\"Content: {content}\\nQuestion: {question}\\n\"\n",
    "    # len = min(len(response),int(response_len))\n",
    "\n",
    "    user_input = f\"Question: {system_prompt}\\n Website: {message['text']}\\n File:\\n{'\\n'.join(file_names)}\"\n",
    "\n",
    "    if validators.url(message['text']):\n",
    "        loader = WebBaseLoader(\n",
    "            # web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "            web_paths=(message['text'],),\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        all_splits = text_splitter.split_documents(docs)\n",
    "        # Index chunks\n",
    "        _ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "        # # # Compile application and test\n",
    "        # graph_builder_i = StateGraph(State).add_sequence([retrieve, generate])\n",
    "        # graph_builder_i.add_edge(START, \"retrieve\")\n",
    "        # graph_i = graph_builder_i.compile()\n",
    "        reply = graph.invoke({\"question\": system_prompt})\n",
    "        response_i = reply[\"answer\"]\n",
    "    elif files:\n",
    "        f = files[-1]\n",
    "        f_name = file_names[-1]多久维护一次产品?\n",
    "        response_i = f\"File: {f_name}\\n\"\n",
    "\n",
    "    # response_i = user_input\n",
    "    for i in range(min(len(response_i), int(tokens))):\n",
    "        time.sleep(0.05)\n",
    "        yield response_i[: i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20e2a1-b622-4970-9069-0202ce10a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "demo = gr.ChatInterface(\n",
    "    answer,\n",
    "    type=\"messages\",\n",
    "    title=\"智能问答RAG\",\n",
    "    description=\"输入一个网址，查询或询问其中的内容。\",\n",
    "    textbox=gr.MultimodalTextbox(value=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                                 file_count=\"multiple\",\n",
    "                                 file_types=[\"image\", \".pdf\", \".txt\"],\n",
    "                                 sources=[\"upload\", \"microphone\"]),\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(\"What is Task Decomposition?\", label=\"你的问题在此输入！\"),\n",
    "        gr.Slider(10,400,value=300,label=\"回答长度\")\n",
    "    ],\n",
    "    multimodal=True,\n",
    ")\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7be72-9389-42cf-91b1-78e8f4bbd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only necessary in a notebook\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88424f53-cd78-41fe-9e06-8a6209001064",
   "metadata": {},
   "source": [
    "## Create a `requirements.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a30aa-9090-460e-acf9-4eb359161125",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../requirements.txt\n",
    "fastcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b2cd7-3123-45bf-945f-882b8a964cf5",
   "metadata": {},
   "source": [
    "## Convert this notebook into a Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706d92c-5785-4f09-9773-b9a944c493a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev.export import nb_export\n",
    "# nb_export('01_gradio.ipynb', lib_path='.', name='gradio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182403f-d1d6-48c0-8e66-46aefb23a9ab",
   "metadata": {},
   "source": [
    "<div>\n",
    "<link rel=\"stylesheet\" href=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.css\">\n",
    "<div id=\"target\"></div>\n",
    "<script src=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.js\"></script>\n",
    "<script>\n",
    "launchGradioFromSpaces(\"abidlabs/question-answering\", \"#target\")\n",
    "</script>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
