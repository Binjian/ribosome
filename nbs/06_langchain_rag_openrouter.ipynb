{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2919343d",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: langchain_rag.html\n",
    "title: Gradio RAG tutorial with langchain\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68340256",
   "metadata": {},
   "outputs": [],
   "source": "# |default_exp langchain_rag"
  },
  {
   "cell_type": "markdown",
   "id": "15fe30a9-cfeb-4ec0-ae40-334072046464",
   "metadata": {},
   "source": [
    "Please reference [this blog post](https://nbdev.fast.ai/blog/posts/2022-11-07-spaces) on how to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7644f-42b3-4d8d-a1d0-f8c3a2358e27",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca22d1e-1bd0-49c0-9b89-c480ad1a29c4",
   "metadata": {},
   "source": [
    "## Make an app with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "id": "00cf4fad-a920-41dc-be42-3992c7fcefac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:07:22.466674Z",
     "start_time": "2025-04-03T06:07:20.719380Z"
    }
   },
   "source": [
    "# |export\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "from openai import api_key\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1cc7c77a267305b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:07:26.353816Z",
     "start_time": "2025-04-03T06:07:23.214323Z"
    }
   },
   "source": [
    "#| export\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.core.llms import ChatMessage\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "9167da2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:07:26.365966Z",
     "start_time": "2025-04-03T06:07:26.359386Z"
    }
   },
   "source": [
    "# |export\n",
    "load_dotenv()\n",
    "#os.environ['HTTP_PROXY'] = ''\n",
    "#os.environ['HTTPS_PROXY'] = ''\n",
    "#os.environ['NO_PROXY'] = 'localhost, 127.0.0.1'\n",
    "print(os.environ.get('OPENROUTER_API_KEY'))\n",
    "print(os.environ.get('OPENROUTER_API_URL'))\n",
    "print(os.environ.get('PINECONE_API_KEY'))\n",
    "print(os.environ.get('HTTP_PROXY'))\n",
    "print(os.environ.get('HTTPS_PROXY'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "https://openrouter.ai/api/v1\n",
      "pcsk_4JQiw5_Kg3hyxuJet9Dumt7zfnXySGp2xZHoQqLsuvHNiTVD98JysmAEqxRbsJEu41Ko7P\n",
      "http://127.0.0.1:20171\n",
      "http://127.0.0.1:20171\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:08:27.158053Z",
     "start_time": "2025-04-03T07:08:27.153485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pinecone import Pinecone, ServerlessSpec, Index\n",
    "import os\n",
    "import pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))"
   ],
   "id": "47e8825809add8ba",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:10:07.195376Z",
     "start_time": "2025-04-03T07:09:59.713983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = 'quickstart'\n",
    "if dataset_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        dataset_name,\n",
    "        dimension=1536,\n",
    "        metric=\"euclidean\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "        )\n",
    "    ),\n",
    "pinecone_index = pc.Index(dataset_name)"
   ],
   "id": "cc5b97fef2bad768",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:13:03.989133Z",
     "start_time": "2025-04-03T07:13:03.974583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
   ],
   "id": "267417287a3eb8c0",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.vector_stores'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mllama_index\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mvector_stores\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpinecone\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PineconeVectorStore\n\u001B[32m      2\u001B[39m vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'llama_index.vector_stores'"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:01:12.665101Z",
     "start_time": "2025-04-03T07:01:10.482312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Embed data\n",
    "data = [\n",
    "    {\"id\": \"vec1\", \"text\": \"Apple is a popular fruit known for its sweetness and crisp texture.\"},\n",
    "    {\"id\": \"vec2\", \"text\": \"The tech company Apple is known for its innovative products like the iPhone.\"},\n",
    "    {\"id\": \"vec3\", \"text\": \"Many people enjoy eating apples as a healthy snack.\"},\n",
    "    {\"id\": \"vec4\", \"text\": \"Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.\"},\n",
    "    {\"id\": \"vec5\", \"text\": \"An apple a day keeps the doctor away, as the saying goes.\"},\n",
    "]\n",
    "\n",
    "embeddings = pc.inference.embed(\n",
    "    model=\"llama-text-embed-v2\",\n",
    "    inputs=[d['text'] for d in data],\n",
    "    parameters={\n",
    "        \"input_type\": \"passage\"\n",
    "    }\n",
    ")\n",
    "\n",
    "vectors = []\n",
    "for d, e in zip(data, embeddings):\n",
    "    vectors.append({\n",
    "        \"id\": d['id'],\n",
    "        \"values\": e['values'],\n",
    "        \"metadata\": {'text': d['text']}\n",
    "    })"
   ],
   "id": "2ef2dd1653a4c24d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:01:17.903444Z",
     "start_time": "2025-04-03T07:01:16.095496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index.upsert(\n",
    "    vectors=vectors,\n",
    "    namespace=\"ns1\"\n",
    ")"
   ],
   "id": "fbd92364080e7cc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 5}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:04:20.885999Z",
     "start_time": "2025-04-03T07:04:19.914771Z"
    }
   },
   "cell_type": "code",
   "source": "index.describe_index_stats()",
   "id": "726b3dfa59e82b65",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'euclidean',\n",
       " 'namespaces': {'ns1': {'vector_count': 5}},\n",
       " 'total_vector_count': 5,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "1b77db9df3d720ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T03:40:24.749965Z",
     "start_time": "2025-04-03T03:40:24.746479Z"
    }
   },
   "source": [
    "llm = OpenRouter(\n",
    "    api_key=os.environ.get('OPENROUTER_API_KEY'),\n",
    "    max_tokens=256,\n",
    "    context_window=4096,\n",
    "    model=\"qwen/qwen2.5-vl-32b-instruct:free\"\n",
    "    # model=\"deepseek/deepseek-r1:free\"\n",
    "    # model=\"gryphe/mythomax-l2-13b:free\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T05:37:47.072789Z",
     "start_time": "2025-04-03T05:37:25.931639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from os import getenv\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=os.environ.get('OPENROUTER_API_URL'),\n",
    "    api_key=os.environ.get('OPENROUTER_API_KEY'),\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    # model=\"bytedance-research/ui-tars-72b:free\",\n",
    "    model=\"google/gemini-2.5-pro-exp-03-25:free\",\n",
    "    extra_headers={\n",
    "        \"HTTP-Referer\": \"binjian.github.io\",\n",
    "        \"X-Title\": \"My Test\",\n",
    "    },\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"ç»™æˆ‘è®²ä¸ªå·æ™®å’Œæ™®äº¬çš„ç¬‘è¯å§.\"\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "id": "3ff680b3471679e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºå·æ™®å’Œæ™®äº¬çš„ç¬‘è¯ï¼š\n",
      "\n",
      "æœ‰ä¸€å¤©ï¼Œå·æ™®å’Œæ™®äº¬ä¸€èµ·åœ¨æ²³è¾¹é’“é±¼ã€‚\n",
      "\n",
      "å·æ™®ç”¨çš„æ˜¯æœ€é«˜ç§‘æŠ€çš„é±¼ç«¿ï¼Œæœ€è´µçš„é±¼é¥µï¼Œæ—è¾¹è¿˜æ”¾ç€ä¸€ä¸ªå·¨å¤§çš„å†·è—ç®±ï¼Œä»–ä¸åœåœ°è·Ÿæ™®äº¬å¹å˜˜ï¼šâ€œå¼—æ‹‰åŸºç±³å°”ï¼Œä½ çœ‹æˆ‘è¿™è£…å¤‡ï¼Œé¡¶çº§çš„ï¼ç»å¯¹èƒ½é’“ä¸Šæœ€å¤§çš„é±¼ï¼æ²¡æœ‰äººæ¯”æˆ‘æ›´æ‡‚é’“é±¼ï¼Œç›¸ä¿¡æˆ‘ï¼â€\n",
      "\n",
      "æ™®äº¬åªæ˜¯é»˜é»˜åœ°ååœ¨é‚£é‡Œï¼Œç”¨ä¸€æ ¹çœ‹èµ·æ¥å¾ˆæ™®é€šçš„æ—§é±¼ç«¿ï¼Œæ—¶ä¸æ—¶è¿˜å¾€æ²³é‡ŒçŸå‡ çœ¼ã€‚\n",
      "\n",
      "è¿‡äº†ä¸€ä¼šå„¿ï¼Œæ™®äº¬ä¸€æ¡æ¥ä¸€æ¡åœ°é’“ä¸Šå¤§é±¼ï¼Œé±¼æ¡¶éƒ½å¿«æ»¡äº†ã€‚è€Œå·æ™®é‚£è¾¹ï¼Œæµ®æ ‡åŠ¨éƒ½æ²¡åŠ¨ä¸€ä¸‹ã€‚\n",
      "\n",
      "å·æ™®æ°”å¾—è„¸éƒ½çº¢äº†ï¼ŒæŠ±æ€¨é“ï¼šâ€œè¿™ä¸å…¬å¹³ï¼è¿™ç»å¯¹æ˜¯è¢«æ“æ§äº†ï¼è¿™äº›é±¼è‚¯å®šæ˜¯å‡é±¼ï¼ˆFake Fishï¼‰ï¼æˆ–è€…æ˜¯æ·±æ°´å·ï¼ˆDeep State\n",
      "ï¼‰æ´¾æ¥çš„é±¼ï¼â€\n",
      "\n",
      "æ™®äº¬æ·¡æ·¡åœ°çœ‹äº†ä»–ä¸€çœ¼ï¼ŒæŠŠä¸€æ¡åˆšé’“ä¸Šæ¥çš„å¤§é²ˆé±¼æ‰”è¿›æ¡¶é‡Œï¼Œå¹³é™åœ°è¯´ï¼šâ€œå”çº³å¾·ï¼Œä¹Ÿè®¸â€¦â€¦é±¼åªæ˜¯æ›´å–œæ¬¢é‚£äº›èƒ½å®‰é™ç­‰å¾…ã€å¹¶ä¸”çŸ¥é“åœ¨å“ªé‡Œä¸‹é’©çš„æ¸”å¤«ï¼Ÿâ€\n",
      "\n",
      "---\n",
      "**æ¸©é¦¨æç¤ºï¼š** è¿™åªæ˜¯ä¸€ä¸ªåŸºäºå…¬ä¼—äººç‰©åˆ»æ¿å°è±¡çš„ç¬‘è¯ï¼Œç›®çš„æ˜¯ä¸ºäº†å¨±ä¹ï¼Œå¹¶ä¸ä»£è¡¨ä»–ä»¬çœŸå®çš„æƒ…å†µæˆ–å…³ç³»ã€‚å¸Œæœ›ä½ è§‰å¾—å¥½ç¬‘ï¼\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T05:49:19.007969Z",
     "start_time": "2025-04-03T05:49:17.212271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = OpenRouter(\n",
    "    api_key=os.environ.get('OPENROUTER_API_KEY'),\n",
    "    max_tokens=256,\n",
    "    context_window=4096,\n",
    "    model=\"google/gemma-3-12b-it:free\"\n",
    "    # model=\"google/gemini-2.5-pro-exp-03-25:free\"\n",
    "    # model=\"deepseek/deepseek-r1:free\"\n",
    "    # model=\"gryphe/mythomax-l2-13b:free\"\n",
    ")\n",
    "message = ChatMessage(role=\"user\", content=\"Tell me a joke.\")\n",
    "resp = llm.chat([message])\n",
    "print(resp)"
   ],
   "id": "58185c30e957ddc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: \n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything! ğŸ˜‚\n",
      "\n",
      "\n",
      "\n",
      "Hope that gave you a chuckle!\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T05:52:35.462713Z",
     "start_time": "2025-04-03T05:52:29.461988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "message = ChatMessage(role=\"user\", content=\"è¯·è®²è¿°ä¸€ä¸ª250å­—çš„ç§‘å¹»å°è¯´æ•…äº‹\")\n",
    "resp = llm.stream_chat([message])\n",
    "for r in resp:\n",
    "    print(r.delta, end='', flush=True)\n"
   ],
   "id": "32993f368452573a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## é—å¿˜ä¹‹æµ·\n",
      "\n",
      "**ç¬¬ä¸€ç« ï¼šé”ˆèš€çš„ä¿¡å·**\n",
      "\n",
      "æ˜Ÿå†3247å¹´ï¼Œæˆ‘ï¼Œè‰¾ç‘æ–¯Â·ç§‘å°”ï¼Œæ˜¯ä¸€åæ˜Ÿé™…è€ƒå¤å­¦å®¶ï¼Œä¸“é—¨ç ”ç©¶å¤±è½æ–‡æ˜çš„é—è¿¹ã€‚æˆ‘çš„é£èˆ¹â€œæ¢å¯»è€…å·â€æ­£æ¼‚æµ®åœ¨é—å¿˜ä¹‹æµ·è¾¹ç¼˜ï¼Œä¸€ä¸ªè¢«æ˜Ÿé™…è”ç›Ÿåˆ’å®šä¸ºç¦åŒºï¼Œå› ä¸ºè¿™é‡Œå……æ–¥ç€æ— æ³•è§£é‡Šçš„èƒ½é‡æ³¢åŠ¨å’Œä»¤äººä¸å®‰çš„å¹»è§‰ã€‚\n",
      "\n",
      "é—å¿˜ä¹‹æµ·ï¼Œæ›¾ç»æ˜¯ç¹æ˜Ÿç‚¹ç‚¹çš„æ˜ŸåŸŸï¼Œå¦‚ä»Šå´åªå‰©ä¸‹ä¸€ç‰‡æ·±é‚ƒçš„é»‘è‰²ï¼Œä»¿ä½›å®‡å®™æœ¬èº«è¢«æ’•è£‚äº†ä¸€é“å£å­ã€‚å‡ ç™¾å¹´å‰ï¼Œä¸€ä¸ªåä¸ºâ€œé˜¿å¡è¿ªäºšâ€çš„æ–‡æ˜çªç„¶æ¶ˆå¤±äºæ­¤ï¼Œç•™ä¸‹çš„åªæœ‰æ— å°½çš„é»‘æš—å’Œä»¤äººæ¯›éª¨æ‚šç„¶çš„ä¼ è¯´ã€‚\n",
      "\n",
      "æˆ‘ä¹‹æ‰€ä»¥æ¥åˆ°è¿™é‡Œï¼Œæ˜¯å› ä¸ºæ”¶åˆ°äº†ä¸€æ®µå¾®å¼±çš„ä¿¡å·ï¼Œä¸€æ®µæ¥è‡ªé˜¿å¡è¿ªäºšæ–‡æ˜çš„æ±‚æ•‘ä¿¡å·ã€‚è”ç›Ÿè®¤ä¸ºè¿™åªæ˜¯å®‡å®™èƒŒæ™¯å™ªéŸ³ï¼Œä½†æˆ‘çš„ç›´è§‰å‘Šè¯‰æˆ‘ï¼Œè¿™å¹¶éå¦‚æ­¤ã€‚é˜¿å¡è¿ªäºšæ–‡æ˜ä»¥å…¶é«˜åº¦å‘è¾¾çš„ç§‘æŠ€å’Œå¯¹ç²¾ç¥é¢†åŸŸçš„æ¢ç´¢è€Œé—»åï¼Œå¦‚æœä»–ä»¬çœŸçš„é­é‡äº†ä»€ä¹ˆï¼Œé‚£å°†æ˜¯è¶³ä»¥æ”¹å˜æ˜Ÿé™…å†å²çš„äº‹ä»¶ã€‚\n",
      "\n",
      "â€œæ¢å¯»"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T05:55:19.092013Z",
     "start_time": "2025-04-03T05:55:13.094961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resp = llm.complete(\"Tell me a joke in Feynman style.\")\n",
    "print(resp)"
   ],
   "id": "a525dce9e6fa9254",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a joke, explained in a Feynman-esque way. It's not *just* the joke, but the process of getting to the joke, and then a little bit of why it's funny, all laid out like Richard Feynman might have approached it.\n",
      "\n",
      "**(Deep breath, adjusts glasses, leans forward slightly)**\n",
      "\n",
      "Alright, so you know how we talk about probability, right? Like, flipping a coin.  It's supposed to be 50/50, heads or tails.  Simple, right?  But *really*, it's a whole mess of physics going on. You've got the initial force you put on it, the angle, the air resistance, the spin... all these things influencing where it lands.  It's chaotic!  You can't *perfectly* predict it.  That's why we use probability â€“ it's a way of dealing with the fact that we don't know all the details.\n",
      "\n",
      "Now, imagine you're flipping a coin, and you flip it, and it lands on heads.  And you flip it again, and it's heads.  And again... heads.  Five heads in a row.  You\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9745999e02e4dca5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = ChatOpenAI(\n",
    "  openai_api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    "  openai_api_base=getenv(\"OPENROUTER_BASE_URL\"),\n",
    "  model_name=\"<model_name>\",\n",
    "  model_kwargs={\n",
    "    \"headers\": {\n",
    "      \"HTTP-Referer\": getenv(\"YOUR_SITE_URL\"),\n",
    "      \"X-Title\": getenv(\"YOUR_SITE_NAME\"),\n",
    "    }\n",
    "  },\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "print(llm_chain.run(question))"
   ],
   "id": "454611c4239fe93e"
  },
  {
   "cell_type": "code",
   "id": "7fedc9815862b597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T02:57:06.964894Z",
     "start_time": "2025-04-03T02:56:56.759636Z"
    }
   },
   "source": [
    "message = ChatMessage(role=\"user\", content=\"ç»™æˆ‘è®²ä¸ªå°‘æ—å¯ºç¬‘è¯å§.\")\n",
    "resp = llm.chat(messages=[message])\n",
    "print(resp)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: å¥½çš„ï¼Œæˆ‘ç»™ä½ è®²ä¸€ä¸ªå…³äºå°‘æ—å¯ºçš„ç¬‘è¯ï¼š\n",
      "\n",
      "---\n",
      "\n",
      "æœ‰ä¸€å¤©ï¼Œå°‘æ—å¯ºçš„æ–¹ä¸ˆå¬é›†ä¼—å¼Ÿå­ï¼Œè¯´ï¼šâ€œä»Šå¤©æˆ‘è¦è€ƒè€ƒä½ ä»¬çš„æ™ºæ…§ã€‚æˆ‘ç»™ä½ ä»¬æ¯äººä¸€ä¸ªæœ¨é±¼ï¼Œè°èƒ½åœ¨æœ€çŸ­çš„æ—¶é—´å†…è®©æœ¨é±¼å‘å‡ºå£°éŸ³ï¼Œè°å°±èƒ½å¾—åˆ°å¥–åŠ±ã€‚â€\n",
      "\n",
      "ä¼—å¼Ÿå­ä¸€å¬ï¼Œçº·çº·å¼€å§‹æƒ³åŠæ³•ã€‚æœ‰çš„å¼Ÿå­æŠŠæœ¨é±¼æ”¾åœ¨çŸ³å¤´ä¸Šæ•²ï¼Œæœ‰çš„å¼Ÿå­ç”¨æœ¨æ£æ•²ï¼Œè¿˜æœ‰çš„å¼Ÿå­ç”šè‡³æŠŠæœ¨é±¼æ‰”åˆ°åœ°ä¸Šæ‘”ã€‚\n",
      "\n",
      "è¿™æ—¶ï¼Œä¸€ä¸ªå°å’Œå°šç«™äº†å‡ºæ¥ï¼Œä»–æ‹¿èµ·æœ¨é±¼ï¼Œæ”¾åœ¨è€³è¾¹ï¼Œè®¤çœŸåœ°å¬äº†ä¸€ä¼šå„¿ï¼Œç„¶åå¯¹å¤§å®¶è¯´ï¼šâ€œæˆ‘å¬åˆ°å£°éŸ³äº†ï¼â€\n",
      "\n",
      "ä¼—å¼Ÿå­éƒ½æ„£ä½äº†ï¼Œçº·çº·é—®ï¼šâ€œä½ æ€ä¹ˆå¬åˆ°çš„ï¼Ÿâ€\n",
      "\n",
      "å°å’Œå°šå¾—æ„åœ°è¯´ï¼šâ€œå¾ˆç®€å•å•Šï¼Œæˆ‘ç”¨è€³æœµå¬çš„ï¼â€\n",
      "\n",
      "æ–¹ä¸ˆå¬äº†ï¼Œå“ˆå“ˆå¤§ç¬‘ï¼Œè¯´ï¼šâ€œä½ è¿™ä¸ªå°å’Œå°šï¼ŒçœŸæ˜¯èªæ˜ï¼â€\n",
      "\n",
      "---\n",
      "\n",
      "è¿™ä¸ªç¬‘è¯è™½ç„¶ç®€å•ï¼Œä½†å¯“æ„æ·±åˆ»ï¼Œæé†’æˆ‘ä»¬æœ‰æ—¶å€™è§£å†³é—®é¢˜çš„æ–¹æ³•å¯èƒ½å¹¶ä¸å¤æ‚ï¼Œå…³é”®åœ¨äºæ¢ä¸ªè§’åº¦æ€è€ƒã€‚å¸Œæœ›ä½ å–œæ¬¢ï¼ ğŸ˜„\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93763aa0099a0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afffa0b24b89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233045d32dc09b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a24da5a1a14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "import validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4d67c64d6c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096eb913c1404ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083cd73fabb9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def answer(message, history, system_prompt, tokens):\n",
    "    files = []\n",
    "    file_names = []\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\" and isinstance(msg[\"content\"], tuple):\n",
    "            files.append(msg[\"content\"][0])\n",
    "            file_names.append(msg[\"content\"][0].split(\"/\")[-1])\n",
    "    for file in message[\"files\"]:\n",
    "        files.append(file)\n",
    "        file_names.append(file.split(\"/\")[-1])\n",
    "\n",
    "    #if message[\"text\"]:\n",
    "    #    content = message[\"text\"]\n",
    "    #else:\n",
    "    #    content = system_prompt\n",
    "    # content = message\n",
    "    # question = system_prompt\n",
    "    # response = f\"Content: {content}\\nQuestion: {question}\\n\"\n",
    "    # len = min(len(response),int(response_len))\n",
    "\n",
    "    user_input = f\"Question: {system_prompt}\\n Website: {message['text']}\\n File:\\n{'\\n'.join(file_names)}\"\n",
    "\n",
    "    if validators.url(message['text']):\n",
    "        loader = WebBaseLoader(\n",
    "            # web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "            web_paths=(message['text'],),\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        all_splits = text_splitter.split_documents(docs)\n",
    "        # Index chunks\n",
    "        _ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "        # # # Compile application and test\n",
    "        # graph_builder_i = StateGraph(State).add_sequence([retrieve, generate])\n",
    "        # graph_builder_i.add_edge(START, \"retrieve\")\n",
    "        # graph_i = graph_builder_i.compile()\n",
    "        reply = graph.invoke({\"question\": system_prompt})\n",
    "        response_i = reply[\"answer\"]\n",
    "    elif files:\n",
    "        f = files[-1]\n",
    "        f_name = file_names[-1]å¤šä¹…ç»´æŠ¤ä¸€æ¬¡äº§å“?\n",
    "        response_i = f\"File: {f_name}\\n\"\n",
    "\n",
    "    # response_i = user_input\n",
    "    for i in range(min(len(response_i), int(tokens))):\n",
    "        time.sleep(0.05)\n",
    "        yield response_i[: i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20e2a1-b622-4970-9069-0202ce10a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "demo = gr.ChatInterface(\n",
    "    answer,\n",
    "    type=\"messages\",\n",
    "    title=\"æ™ºèƒ½é—®ç­”RAG\",\n",
    "    description=\"è¾“å…¥ä¸€ä¸ªç½‘å€ï¼ŒæŸ¥è¯¢æˆ–è¯¢é—®å…¶ä¸­çš„å†…å®¹ã€‚\",\n",
    "    textbox=gr.MultimodalTextbox(value=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                                 file_count=\"multiple\",\n",
    "                                 file_types=[\"image\", \".pdf\", \".txt\"],\n",
    "                                 sources=[\"upload\", \"microphone\"]),\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(\"What is Task Decomposition?\", label=\"ä½ çš„é—®é¢˜åœ¨æ­¤è¾“å…¥ï¼\"),\n",
    "        gr.Slider(10,400,value=300,label=\"å›ç­”é•¿åº¦\")\n",
    "    ],\n",
    "    multimodal=True,\n",
    ")\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7be72-9389-42cf-91b1-78e8f4bbd083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7865\n"
     ]
    }
   ],
   "source": [
    "# this is only necessary in a notebook\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88424f53-cd78-41fe-9e06-8a6209001064",
   "metadata": {},
   "source": [
    "## Create a `requirements.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a30aa-9090-460e-acf9-4eb359161125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../requirements.txt\n",
    "fastcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b2cd7-3123-45bf-945f-882b8a964cf5",
   "metadata": {},
   "source": [
    "## Convert this notebook into a Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706d92c-5785-4f09-9773-b9a944c493a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev.export import nb_export\n",
    "# nb_export('01_gradio.ipynb', lib_path='.', name='gradio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182403f-d1d6-48c0-8e66-46aefb23a9ab",
   "metadata": {},
   "source": [
    "<div>\n",
    "<link rel=\"stylesheet\" href=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.css\">\n",
    "<div id=\"target\"></div>\n",
    "<script src=\"https://gradio.s3-us-west-2.amazonaws.com/2.6.5/static/bundle.js\"></script>\n",
    "<script>\n",
    "launchGradioFromSpaces(\"abidlabs/question-answering\", \"#target\")\n",
    "</script>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
